{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "In this lecture, we'll explore PCA, one of the most commonly used unsupervised machine learning methods. We'll cover the motivation behind PCA, its mathematical foundations, and a comprehensive practical application on a real-world dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "High-dimensional data is ubiquitous in many fields. However, high-dimensionality can lead to problems such as overfitting and difficulty in visualizing data. PCA helps by reducing the dimensionality of the dataset while retaining the most important information (i.e. the variance). \n",
    "\n",
    "**Key Applications of PCA:**\n",
    "- **Data Visualization:** Reduce data to 2 or 3 dimensions to visualize clusters and structure.\n",
    "- **Noise Reduction:** Eliminate redundant/noisy features.\n",
    "- **Feature Extraction:** Create new and, most importantly, __uncorrelated__ variables (principal components) that represent the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Foundations of PCA\n",
    "\n",
    "<figure>\n",
    "    <img src=\"images/PCA.jpg\" width=\"400\" alt=\"PCA visualization\">\n",
    "    <figcaption>Source: Lavrenko and Sutton 2011</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "PCA is an widely used technique in modern machine learning. The main steps usually include:\n",
    "\n",
    "1. **Standardization:** Since PCA is affected by scale, we center and scale the data so each feature contributes equally.\n",
    "\n",
    "2. **Covariance Matrix Computation:** This matrix captures the relationships between features. For data matrix \\(X\\), the covariance matrix is computed as:\n",
    "\n",
    "$$\n",
    "Cov(X) = \\frac{1}{n-1} (X - \\bar{X})^T (X - \\bar{X})\n",
    "$$\n",
    "\n",
    "\n",
    "3. **Eigen Decomposition:** We compute the eigenvalues and eigenvectors of the covariance matrix. The eigenvectors represent the directions (principal components) and the eigenvalues indicate the amount of variance captured by each component.\n",
    "\n",
    "4. **Selection of Principal Components:** Components are ranked by eigenvalue. The top \\(k\\) components that capture the majority of the variance are selected.\n",
    "\n",
    "5. **Projection:** The original data is projected onto these principal components, yielding a lower-dimensional representation.\n",
    "\n",
    "Below, we go through each of these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding PCA with Synthetic Data\n",
    "\n",
    "Let's begin by building an intuitive understanding of PCA using simple two-dimensional data.\n",
    "\n",
    "We'll generate two correlated features and visualize them. Then, we'll identify the direction of maximum variance (the first principal component) and rotate the data into the new coordinate system what happen to the axes defining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Generate synthetic data with two correlated features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = np.column_stack((x, y))\n",
    "\n",
    "# Plot the synthetic data\n",
    "plt.scatter(data[:,0], data[:,1], alpha=0.7)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit a PCA model to the data and visualize the principal components. We can print the explained variance ratio, which tells us how much variance is captured by each component. We can also print the eigenvectors and eigenvalues of the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA on the synthetic data\n",
    "\n",
    "\n",
    "print('Variances (eigenvalues): ', pca.explained_variance_)\n",
    "print('Explained variance ratio:', pca.explained_variance_ratio_)\n",
    "print('Principal components (eigenvectors):\\n', pca.components_)\n",
    "\n",
    "# Extract the first principal componenta and the mean of the data\n",
    "\n",
    "\n",
    "\n",
    "print('Mean of data:', mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot the data once again and highlight the first principal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the principal component direction on the original data\n",
    "plt.scatter(data[:,0], data[:,1], alpha=0.7, label='Data points')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "\n",
    "# Scale the PC vector for plotting\n",
    "scale = 3 * np.std(data, axis=0).max()\n",
    "plt.arrow(mean[0], mean[1], scale*pc1[0], scale*pc1[1], color='red', width=0.02, head_width=0.1, label='PC1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's rotate the data into the new coordinate system defined by the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate the data into principal component space\n",
    "\n",
    "\n",
    "\n",
    "# Plot the data in the PCA (rotated) space\n",
    "\n",
    "\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Data in Principal Component Space (Uncorrelated)')\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.axvline(0, color='red', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this simple example, we generated two correlated features. We then computed the principal component direction, which captures the highest variance, and rotated the data into this new coordinate system. Notice that in the transformed space, the axes (PC1 and PC2) are uncorrelated, which is one of the key benefits of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA Example on the Digits Dataset\n",
    "\n",
    "Now, let's apply PCA to a real-world dataset. We will use the `load_digits` dataset, which consists of 8x8 images of handwritten digits (64 features per image). In this section, we'll:\n",
    "\n",
    "- Load and standardize the data.\n",
    "- Compute the covariance matrix and perform eigen decomposition (via scikit-learn's PCA).\n",
    "- Visualize the explained variance using a Scree Plot.\n",
    "- Project the data to 2 dimensions and visualize the clusters corresponding to different digits.\n",
    "- Visualize the principal components as images to see the dominant features.\n",
    "\n",
    "\n",
    "Let's starting by loading the data as we've been doing in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the digits dataset\n",
    "\n",
    "\n",
    "print('Original data shape:', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's standardize the data. We can use the `StandardScaler` from scikit-learn to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "\n",
    "\n",
    "print('Mean after scaling (first 5 features):', np.mean(X_scaled, axis=0)[:5])\n",
    "print('Std after scaling (first 5 features):', np.std(X_scaled, axis=0)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is standardized, we can fit a PCA model to the data. We can then print the explained variance ratio and plot the Scree Plot to visualize the explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the covariance matrix and perform eigen decomposition manually (optional demonstration)\n",
    "\n",
    "\n",
    "print('Covariance matrix shape:', cov_matrix.shape)\n",
    "\n",
    "\n",
    "\n",
    "print('Top 5 Eigenvalues:', eigen_values[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA from scikit-learn to generate the Scree Plot\n",
    "\n",
    "\n",
    "explained_variance = pca_full.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(1, len(explained_variance)+1), explained_variance, alpha=0.6, label='Individual Explained Variance')\n",
    "plt.step(range(1, len(cumulative_variance)+1), cumulative_variance, where='mid', label='Cumulative Explained Variance')\n",
    "plt.xlabel('Principal Component Index')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot for Digits Dataset')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "print('Cumulative variance for first 10 components:', cumulative_variance[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same, but this time we'll project the data to 2 dimensions and visualize the clusters corresponding to different digits.\n",
    "\n",
    "Finally, we can visualize the principal components as images to see the dominant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce the digits data to 2 dimensions for visualization\n",
    "\n",
    "\n",
    "print('Shape of PCA-transformed data:', X_pca.shape)\n",
    "print('Explained variance ratio (2 components):', pca.explained_variance_ratio_)\n",
    "print('Total variance explained:', np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 2D PCA projection of the digits dataset\n",
    "plt.figure(figsize=(8,6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', alpha=0.8)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('2D PCA Projection of Digits Dataset')\n",
    "plt.colorbar(scatter, label='Digit Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One really interesting thing we can do is to visualize the PCs as images. This can give us an idea of the dominant features that the PCs are capturing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the top 10 principal components as images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
    "components = pca_full.components_[:10]  \n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(components[i].reshape(8,8), cmap='gray')\n",
    "    ax.set_title(f'PC {i+1}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Top 10 Principal Components as Images')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the data projected onto the first _n_ principal components. This can help us understand how the data is distributed in the lower-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 10 principal components\n",
    "pca_10 = PCA(n_components=20)\n",
    "X_pca_10 = pca_10.fit_transform(X_scaled)\n",
    "X_reconstructed_scaled = pca_10.inverse_transform(X_pca_10)\n",
    "X_reconstructed = scaler.inverse_transform(X_reconstructed_scaled)\n",
    "\n",
    "# Create a grid: top row original, bottom row reconstructed for digits 0-9\n",
    "fig, axes = plt.subplots(2, 10, figsize=(15, 4))\n",
    "for digit in range(10):\n",
    "    idx = np.where(y == digit)[0][0]\n",
    "    axes[0, digit].imshow(X[idx].reshape(8,8), cmap='gray')\n",
    "    axes[0, digit].set_title(f\"{digit}\")\n",
    "    axes[0, digit].axis('off')\n",
    "    \n",
    "    axes[1, digit].imshow(X_reconstructed[idx].reshape(8,8), cmap='gray')\n",
    "    axes[1, digit].set_title(\"Reconstructed\")\n",
    "    axes[1, digit].axis('off')\n",
    "    \n",
    "plt.suptitle(\"Original vs. Reconstructed Digits (0-9) using 10 Principal Components\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Together, these examples demonstrate how PCA simplifies data, aids visualization, and provides insights into the underlying structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5be301c4-61af-4f40-be5f-57f2ba4b7733",
   "metadata": {},
   "source": [
    "# Natural Language Processing Intro\n",
    "\n",
    "> Some of the examples in this notebook are adapted from the following sources:\n",
    "> * NVIDIA DLI: [Building Transformer-Based Natural Language Processing Applications](https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+C-FX-03+V3). This is an instructor-led course that I teach periodically--let me know if you have a group that would be interested in taking this workshop.\n",
    "> * LinkedIn Learning: [Introduction to Transformer Models for NLP by Sinan Ozdemir](https://www.linkedin.com/learning/introduction-to-transformer-models-for-nlp/introduction). This is a great course, and like all LinkedIn Learning courses, is free to UF students, faculty and staff.\n",
    "\n",
    "\n",
    "Natural Language Processing (NLP) is a large field of AI with many related but somewhat distinct sub-disciplines. We won't have time to look at all of these, but you are most likely somewhat familiar with many of the applications.\n",
    "\n",
    "Some NLP Tasks:\n",
    "* Summary generation, information extractions\n",
    "* Translation (language to language)\n",
    "* Transcription (speech to sample_text)\n",
    "* Auto-completion\n",
    "* Sentiment Analysis\n",
    "* Intent Detection\n",
    "* Voice assistant\n",
    "* Document retrieval\n",
    "* And the BIG one these days, **Chat**, automated writing, dialog generation, question answering, etc., the kinds of things that ChatGPT is known for. \n",
    "\n",
    "## Ambiguity in language\n",
    "\n",
    "NLP is not a simple task, and until deep learning, was quite limited in its abilities. As with most fields in AI, there is a long history [dating back to the 1950s](https://en.wikipedia.org/wiki/Natural_language_processing). Part of the challenge is that human language tends to be ambiguous, and recognizing words is really only the start of inferring meaning. Take for example, this sentence:\n",
    "\n",
    "   > The boy saw a man with a telescope\n",
    "   \n",
    "   * Who had the telescope?\n",
    "   \n",
    "More consample_text is needed to answer this. Yet, consample_text is not always there, and even when it is, it can be a challenge for NLP methods (and even human readers at times).\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "One of the challenges of NLP is representing language as numbers--remember, computers and the ML/AI systems that we have primarily deal with numbers. This was relatively easy for computer vision problems because we took the pixel intensities of an image and fed those into our models. But what should we do with speech, words, and sample_text?\n",
    "\n",
    "The process of converting sample_text to numerical representation is called **tokenization**. There are many methods of tokenization, but the idea is to break sample_text into itemizable components--tokens.\n",
    "\n",
    "Tokens can be words, letters, word fragments, or even sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f3b29b",
   "metadata": {},
   "source": [
    "## The BERT WordPiece Tokenizers\n",
    "\n",
    "From the NVIDIA Course:\n",
    "> Tokenization splits a word, phrase, or larger sample_text section into individual characters, words, or subwords.  For example, the word \"tokenization\" could be split in a number of ways:\n",
    ">\n",
    "> * Characters: 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n'\n",
    "> * Words: 'tokenization'\n",
    "> * Subwords: 'token', '##ization'\n",
    ">\n",
    "> The idea is to create a vocabulary of tokens from a sample_text corpus, which can then be trained in a language model to characterize language relationships between the tokens.  Whether this is done by character, word, or subword affects the complexity of the problem.\n",
    ">\n",
    "> Tokenization by characters has the advantage of a very limited number of tokens to deal with, but these few tokens are not very meaningful by themselves and long sequences of tokens are required to represent sample_text.  Tokenization by words results in a very large vocabulary size and requires separate tokens for very similar words, which in turn requires more training to determine their relationships to each other.\n",
    ">\n",
    "> Tokenization by subwords is a solution that tries to balance these two. For example, the word \"token\" is a subword for \"tokenization\", \"tokens\", and \"tokenize\".  By splitting the words, the model learns similar meanings from the same root word more easily.  The size of the overall vocabulary required for understanding is less than required for word tokenization.\n",
    "\n",
    "## WordPiece Algorithm\n",
    "> The WordPiece algorithm was introduced in [this paper by Schuster and Nakajima](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf).  To begin, the training data (corpus) is chosen, as well as the subword vocabulary size desired.   The algorithm iteratively determines optimal subwords for the body of sample_text and creates the vocabulary with assigned values.  The iterative steps are:\n",
    ">\n",
    "> 1. Split words into sequences of character tokens.\n",
    "> 2. Build the language model on the training data using tokens from previous step.\n",
    "> 3. Generate new unit tokens by combining two tokens with high likelihood in the language model and add the new token(s) to the vocabulary.\n",
    "> 4. Repeat from step 2 until the token limit for the desired vocabulary is reached or the likelihood falls below some desired threshold.\n",
    "\n",
    "Let's look at this in code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ce73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nemo nlp collection \n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "\n",
    "# Import BERT\n",
    "from nemo.collections.nlp.models import BERTLMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98970194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the list of pre-trained BERT language models\n",
    "BERTLMModel.list_available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b73d1d-fd5a-4cc5-b60e-2cbf0177502b",
   "metadata": {},
   "source": [
    "> There are two pretrained BERT language models available with NeMo: \n",
    "> - `bertbaseuncased` model has 110 millions parameters in total with 12 Transformer blocks.\n",
    "> - `bertlargeuncased` model has 340 millions parameters in total with 24 Transformer blocks.\n",
    "> \n",
    "> For the sake of time and simplicity, we'll download the smaller variant, BERT Base. This could take a minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a6839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pretrained BERT-based model\n",
    "pretrained_model_name=\"bertbaseuncased\"\n",
    "model = BERTLMModel.from_pretrained(pretrained_model_name, strict=False) # Pass strict=False to avoid needing to specify\n",
    "                                                                        # training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3910c74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the number of weights in the model\n",
    "print(f'Number of weights in {pretrained_model_name}: {model.num_weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb897d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available tokenizers\n",
    "nemo_nlp.modules.get_tokenizer_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd78347-fd30-41a5-808a-b1da7dc1ba95",
   "metadata": {},
   "source": [
    "As I indicated, there are **a lot** of options on how to do tokenization...though in general, you will need to use what was used in training the model if you use a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11732637-6fa3-4a62-9e1f-1aaa07155d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the bert-base-uncased tokenizer \n",
    "tokenizer_uncased = nemo_nlp.modules.get_tokenizer(tokenizer_name=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcc4b5e-bd4a-4c08-87cf-0472b9a64264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the vocabulary size\n",
    "print(f'The vocabulary size of {pretrained_model_name}: {tokenizer_uncased.vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69722feb-7d4b-4bf0-8ce5-fe3422f75500",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sample_text = \"Hello, my name is Matt. I live in Gainesville, FL.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab212db-8bc2-4d44-9a71-59a6cd203fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_uncased=tokenizer_uncased.text_to_tokens(sample_sample_text)\n",
    "print(f'Input sentence: {sample_sample_text}')\n",
    "print(f'Tokenized sentence: {output_uncased}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51da8e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the bert-base-cased tokenizer \n",
    "tokenizer_cased = nemo_nlp.modules.get_tokenizer(tokenizer_name=\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22565c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the sample_text \n",
    "output_cased=tokenizer_cased.text_to_tokens(sample_sample_text)\n",
    "print(f'Input sentence: {sample_sample_text}')\n",
    "print(f'Tokenized sentence: {output_cased}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797a91ea-d186-4ef7-baef-551508d0d332",
   "metadata": {},
   "source": [
    "> The BERT model [or any NLP model really] does not accept sample_text inputs, but rather their numerical index representations.\n",
    ">\n",
    "> We can check the vocabulary index of a word using the `tokenizer.sample_text_to_ids()` function. \n",
    ">\n",
    "> Try it with the `bert-base-cased` tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2357b-00a8-4ad6-aa4d-759febfaebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the tokens Hello and hello using bert-base-cased tokenizer\n",
    "print(f'Index of Hello: {tokenizer_cased.text_to_ids(\"Hello\")}')\n",
    "print(f'Index of hello: {tokenizer_cased.text_to_ids(\"hello\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39755ed2-2e05-4a72-98a3-0b947e3ce095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of bert-base-cased tokenizer in a sentence\n",
    "print(f'Input sentence: {sample_sample_text}')\n",
    "print(f'Tokenized sentence: {output_uncased}')\n",
    "print(f'Tokenized sentence: {tokenizer_uncased.text_to_ids(sample_sample_text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef736ba-456f-4094-9229-b09eb77dba47",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Embedding\n",
    "\n",
    "The tokenization process takes care of converting words to numbers. This is a first step, but we want to consample_textualize the words--i.e., what does each word mean? Embedding goes a step further, representing words in n-dimensional vector space. This vector space has a lower dimensionality than the number of words in the vocabulary (as opposed to one-hot encoding) and results in words with similar meaning being closer in space than other words.\n",
    "\n",
    "Here's an example figure showing some embeddings (taken from Renu Khandelwal's article [*Word Embeddings for NLP*](https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4):\n",
    "\n",
    "\n",
    "![Image of word embeddings from https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4 ](images/word_embeddings_Renu_Khandelwal.png)\n",
    "\n",
    "The embedding is learned in the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b32991b-10bc-491f-8fa9-8b799b271511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set up the sentence we want to look at\n",
    "sample_text = \"Last night, my wireless mouse was eaten by an animal such as a mouse or rat. I need to order a new optical computer mouse.\"\n",
    "input_sentence=torch.tensor([tokenizer_uncased.tokenizer(sample_text).input_ids]).cuda()\n",
    "attention_mask=torch.tensor([tokenizer_uncased.tokenizer(sample_text).attention_mask]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c2d68b-a554-42f8-83e9-e5d413f19849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the tokenization for the sentence\n",
    "print(f'Input sentence: {sample_text}')\n",
    "output_uncased=tokenizer_uncased.ids_to_tokens(tokenizer_uncased.tokenizer(sample_text).input_ids)\n",
    "print(f'Tokenized sentence: {output_uncased}')\n",
    "\n",
    "# \"mouse\" tokens positions in the sample_text input\n",
    "mouse_computer_1=6\n",
    "mouse_animal=15\n",
    "mouse_computer_2=27\n",
    "\n",
    "print(\"Check that numbers line up:\")\n",
    "print(f'  Should be \"mouse, mouse, mouse\": {output_uncased[mouse_computer_1]}, {output_uncased[mouse_animal]}, {output_uncased[mouse_computer_2]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c6f794-80b6-4211-ab40-6153aefd8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embeddings for the pretrained model\n",
    "hidden_states = model.bert_model(input_ids=input_sentence, token_type_ids=None, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a87cbd7-bf4c-40a4-bc14-7ca94cfc7eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the dimensionality of the embedding\n",
    "len(hidden_states[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59548dc7-1f8a-410d-bacc-796fcff53693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def similarity_cosine(x,y):\n",
    "    return dot(x,y)/(norm(x)*norm(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98537425-4c28-4ade-8f6b-9cbcc3991f50",
   "metadata": {},
   "source": [
    "> We can visualize sample_text token embeddings obtained from the BERT models if we first reduce the dimensionality to 2D. \n",
    "[t-SNE](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) is a dimensionality reduction technique widely used for vector visualization in 2D or 3D, as it preserves the neighborhood distances.\n",
    ">\n",
    "> The following codeblocks uses the [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) implementation of the t-SNE algorithm to reduce the dimensionality of BERT sample_text token embeddings from 768 to 2, and plots the 2D vectors. Note that as t-SNE is a stochastic process, the low dimensional embeddings will vary from one run to another. However, the neighborhood distances of tokens should remain more or less the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb84c7-3e81-4e86-9e85-b326fe1d8ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "X=hidden_states.cpu().detach().numpy()\n",
    "X_embedded = TSNE(n_components=2,metric='euclidean',  init='random', perplexity=7).fit_transform(X[0])\n",
    "Tokens=tokenizer_uncased.ids_to_tokens(tokenizer_uncased.tokenizer(sample_text).input_ids)\n",
    "\n",
    "# Annotate the different mouse tokens\n",
    "Tokens[mouse_computer_1]=\"mouse_computer_1\"\n",
    "Tokens[mouse_animal]=\"mouse_animal\"\n",
    "Tokens[mouse_computer_2]=\"mouse_computer_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc837b6f-99af-4fc7-bbb1-5eb9c9a3c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(X_embedded[:,0],X_embedded[:,1], '.', color='black')\n",
    "plt.plot([X_embedded[mouse_computer_1,0],X_embedded[mouse_animal,0]],[X_embedded[mouse_computer_1,1],X_embedded[mouse_animal,1]],color='red')\n",
    "plt.plot([X_embedded[mouse_computer_1,0],X_embedded[mouse_computer_2,0]],[X_embedded[mouse_computer_1,1],X_embedded[mouse_computer_2,1]],color='green')\n",
    "\n",
    "for i, txt in enumerate(Tokens):\n",
    "    plt.annotate(txt, (X_embedded[i,0], X_embedded[i,1]), color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813a598-e50b-4f9b-a2e1-d10f85c6efdd",
   "metadata": {},
   "source": [
    "## Tokenizers and Embeddings are learned\n",
    "\n",
    "> The tokenizer algorithm generates the vocabulary following variants of Top-K frequent words from the text corpus.\n",
    ">\n",
    "> The vocabulary size is limited because the training cost increases with the size of the vocabulary. Including all unique words from the text corpus into the vocabulary would explode the complexity of training beyond the capabilities of the tokenizer. For instance, the BERT model that was released in 2018, with a subword tokenizer algorithm called WordPiece, has a vocabulary limit of 30,000.\n",
    ">\n",
    "> How, then, do tokenizers deal with terms that are not part of the vocabulary or **out-of-vocabulary (OOV)** words?\n",
    ">\n",
    "> 1. One option is to replace OOV words with a special token, `[UNK].` In this case, all OOV terms will have the same representation for the neural network losing the semantic. \n",
    "> 1. A second option is to split OOV words at the character level. This increases the size of the input to the neural language model, adding the challenge of learning the relationship between characters to keep the semantic.\n",
    "> 1. Sub-word tokenizers, such as BERT WordPiece, provide a solution in between the word token and character split option. It tokenizes OOV words into subwords.\n",
    ">\n",
    "> Let's have a look at the `bert-base-uncased` tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02da9559-ac16-4b97-8a05-8d41f8095b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the bert-base-uncased tokenizer \n",
    "tokenizer_uncased = nemo_nlp.modules.get_tokenizer(tokenizer_name=\"bert-base-uncased\")\n",
    "\n",
    "# get the vocabulary size\n",
    "print(f\"The vocabulary size: {tokenizer_uncased.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379973df-a77a-4914-8ba2-4f09a2b61324",
   "metadata": {},
   "source": [
    "> As an example, take a look at the format tokenization for years with BERT. Years prior to 2021 appear frequently enough in the corpus to be part of the vocabulary, while years in the future are OOV and are split into sub-tokens.\n",
    ">\n",
    "> Try it in the cell below using the `tokenizer_uncased.text_to_tokens()` function for various years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0448c4b-4213-46a6-a77e-5d4d5e014660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert tokenizer for years\n",
    "print(f\"Tokenized year: {tokenizer_uncased.text_to_tokens('2019')}\")\n",
    "print(f\"Tokenized year: {tokenizer_uncased.text_to_tokens('2020')}\")\n",
    "print(f\"Tokenized year: {tokenizer_uncased.text_to_tokens('2021')}\")\n",
    "print(f\"Tokenized year: {tokenizer_uncased.text_to_tokens('2022')}\")\n",
    "print(f\"Tokenized year: {tokenizer_uncased.text_to_tokens('2023')}\")\n",
    "print(f\"Tokenized year: {tokenizer_uncased.text_to_tokens('2030')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826435a3-0bc6-47f5-a725-d9e1190d410e",
   "metadata": {},
   "source": [
    "## Where to next?\n",
    "\n",
    "Initial attempts at language models were largely statistical, trying to find statistical patterns in texts to draw conclusions. In 2013 and 2014, Ilya Sutskever, one of the co-inventors of AlexNet (and now famous for being one of the OpenAI board members who fired Sam Altman in 2023), started applying deep learning to language models.\n",
    "\n",
    "### RNNs and LSTMs\n",
    "\n",
    "In addition to the challenges discussed above, another challenge with language modeling is that the order of words matters. Initial attempts at neural networks for language used Recurrent Neural Networks (RNNs) to try to capture meaning over the sequence of words.\n",
    "\n",
    "![RNN diagram from Wikimedia Commons](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "One challenge that quickly became apparent was that the further apart words were, the harder it was to have their information influence each other. Long Short-Term  Memory (LSTM) networks were developed to try to capture these longer dependencies using remember and forget gates.\n",
    "\n",
    "![LSTM diagrom from Wikimedia Commons](https://upload.wikimedia.org/wikipedia/commons/9/93/LSTM_Cell.svg)\n",
    "\n",
    "This helped some, but the context was still hard to propagate beyond relatively short sentences.\n",
    "\n",
    "### CNNs and Attention\n",
    "\n",
    "By 2015, people were experimenting with using CNNs (yes, the same CNNs we used in computer vision!). The kernel was one-dimensional scanning across the sequence of words. Attention was also introduced as part of CNN methods.\n",
    "\n",
    "### Attention is All You Need\n",
    "\n",
    "As we will see in the next notebook, with the publishing of the 2017 paper introducing transformers, the field was...*transformed*!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d302c8e-7a35-46fc-9735-c65253d5d377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeMo NLP",
   "language": "python",
   "name": "nemo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

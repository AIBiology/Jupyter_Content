{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea163b2c-cd1c-420e-afcf-6a7e05474662",
   "metadata": {},
   "source": [
    "# Introduction to Convolutional Neural Networks, part 2\n",
    "\n",
    "In [part 1](16_CNNs_part1.ipynb) of this section, we loaded the ASL data, visualized some sample images, normalized the data, and created and trained a model. That model had three layed, the first two had 512 neurons and the last had 25 and used the softmax activation function to generate a prediction that was the probablity that an image belonged to each of the 25 categories.\n",
    "\n",
    "With that model, we achieved an accuract of about 80%.\n",
    "\n",
    "We looked at some [slides on convolutional kernels, padding, pooling, dropout and data augmentation](https://docs.google.com/presentation/d/1uSk7xHWZ9H6YihUP4OdHpIVws_2py_HBfbby7GpZDCA/edit?usp=sharing). Now we can implement these.\n",
    "\n",
    "## Reload and pre-process our data as in [part 1](16_CNNs_part1.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f7283-df18-44a7-a7c8-d302ea4bf742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import os \n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "\n",
    "import keras\n",
    "from keras.src.legacy.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image as image_utils\n",
    "\n",
    "\n",
    "\n",
    "from helpers_plot_history import plot_history # Some helper functions for the CNN notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d970832-2721-4a1d-a7c5-38463c740210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "sign_train = pd.read_csv(\"data/sign_mnist/sign_mnist_train.csv\")\n",
    "sign_test = pd.read_csv(\"data/sign_mnist/sign_mnist_test.csv\")\n",
    "\n",
    "# Prepare X and y\n",
    "y_train = sign_train['label']\n",
    "X_train = sign_train.drop(columns='label').values\n",
    "\n",
    "y_test = sign_test['label']\n",
    "X_test = sign_test.drop(columns='label').values\n",
    "\n",
    "# Normalize the data\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "# Convert our classes to categorical\n",
    "num_classes = 25 # Not entirely sure what the 25th category is...\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2013534a-5b93-4246-9e6e-ba024b3a0089",
   "metadata": {},
   "source": [
    "## Add Convolutional Kernels, Max Pooling, and Dropout\n",
    "\n",
    "In part 1, we treated the image data as a row of 784 pixels. But our images are 28 rows of 28 pixels (a matrix of pixels). There is information in that spacial arrangement that is lost by simplifying the data into a row. \n",
    "\n",
    "Most computer vision tasks work best with the images in that matrix format. So, let's transform the data into the 28X28 shape.\n",
    "\n",
    "### Reshape our data into a 28X28 matrix per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31085aab-f5f8-4f4a-b841-6e258ce54247",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape before: {X_train.shape}')\n",
    "X_train = X_train.reshape(-1,28,28,1)\n",
    "X_test= X_test.reshape(-1,28,28,1)\n",
    "\n",
    "print(f'Shape after: {X_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7524f962-7fc9-4d8f-897e-22492aa5d70c",
   "metadata": {},
   "source": [
    "### Our new model\n",
    "\n",
    "Here's a diagram of the model that we'll use. There are *some* theoretical reasons behind this model, but a **lot** of it is \"someone tried it and it worked well\". \n",
    "\n",
    "![Diagram of the model implemented in code below](images/asl_model.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe603442-20f7-42e3-9821-7db1283fd3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    Conv2D,\n",
    "    MaxPool2D,\n",
    "    Flatten,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(75, (3, 3), strides=1, padding=\"same\", activation=\"relu\", \n",
    "                 input_shape=(28, 28, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "model.add(Conv2D(50, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "model.add(Conv2D(25, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=num_classes, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d891b88-71d0-436e-a45e-202ea3308f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365aa33-29e1-4cec-b37e-21c45173bfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b35d6c-5536-47d2-982d-55ab7bef22c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bd6635-c715-4f49-8e9f-f9a13f1d0d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c08dfe7-80b5-4012-93bb-3821fa764662",
   "metadata": {},
   "source": [
    "## Getting better, but still not great\n",
    "### Add some data augmentation\n",
    "\n",
    "This is implemented using a data flow generator. This image from Adrian Rosenbrock's article [Keras ImageDataGenerator and Data Augmentation](https://pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/) is a good summary:\n",
    "\n",
    "![Data flow generator diagram](https://b2633864.smushcdn.com/2633864/wp-content/uploads/2019/07/keras_data_augmentation_in_place.png?lossy=2&strip=1&webp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efed862-a3d5-421a-900e-05e3a070d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the augmentation pipeline\n",
    "data_augmentation = keras.Sequential([\n",
    "  keras.layers.RandomRotation(10/360),         # 10Â° rotation\n",
    "  keras.layers.RandomTranslation(0.1, 0.1),      # width & height shifts of 10%\n",
    "  keras.layers.RandomZoom(0.1, 0.1),             # zoom in/out up to 10%\n",
    "  keras.layers.RandomFlip(\"horizontal\")          # horizontal flip only\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1090f2fc-53d6-406e-aacf-d2b47221e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "augmented_images = data_augmentation(X_train[:batch_size])\n",
    "fig, ax = plt.subplots(4, 8, figsize=(12, 6))\n",
    "for i in range(batch_size):\n",
    "    ax.flatten()[i].imshow(np.squeeze(augmented_images[i].cpu()), cmap='gray')\n",
    "    ax.flatten()[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dfc221-5ce3-426e-ad81-4c54aa2d6367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new model, adding the augmentation\n",
    "aug_model = Sequential()\n",
    "aug_model.add(data_augmentation)\n",
    "aug_model.add(model)\n",
    "\n",
    "# Compile the new model\n",
    "aug_model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "aug_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bc50dc-e62c-41c2-ab5d-7c207dd40eba",
   "metadata": {},
   "source": [
    "From the Nvidia notebooks:\n",
    "\n",
    "> When using an image data generator with Keras, a model trains a bit differently: instead of just passing the `[X]_train` and `y_train` datasets into the model, we pass the generator in, calling the generator's [flow](https://keras.io/api/preprocessing/image/) method. This causes the images to get augmented live and in memory right before they are passed into the model for training.\n",
    ">\n",
    "> Generators can supply an indefinite amount of data, and when we use them to train our data, we need to explicitly set how long we want each epoch to run, or else the epoch will go on indefinitely, with the generator creating an indefinite number of augmented images to provide the model.\n",
    ">\n",
    "> We explicitly set how long we want each epoch to run using the `steps_per_epoch` named argument. Because `steps * batch_size = number_of_images_trained in an epoch` a common practice, that we will use here, is to set the number of steps equal to the non-augmented dataset size divided by the batch_size (which has a default value of 32).\n",
    ">\n",
    "> Run the following cell to see the results. The training will take longer than before, which makes sense given we are now training on more data than previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7688a2-6963-4b57-b2ed-caf76504a96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = aug_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=6,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb805f4a-4718-43d4-b08a-0819e58daabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41bd1dd-fffd-4a85-ba95-975542c10277",
   "metadata": {},
   "source": [
    "## Let's see how well the model does with new data\n",
    "\n",
    "Let's play with some new images that aren't necessarily in the format the model was trained on. First we'll need to modify the new images to match what the model was trained on. The functions below help with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d0f643-2d51-4f11-b9d9-7867679fcaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image_path):\n",
    "    '''Shows the image at a given path as it is.'''\n",
    "    image = mpimg.imread(image_path)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "\n",
    "def load_and_scale_image(image_path):\n",
    "    '''Loads and scales the image to a 28x28 greyscale image, like the training data'''\n",
    "    image = image_utils.load_img(image_path, color_mode=\"grayscale\", target_size=(28,28))\n",
    "    return image\n",
    "\n",
    "# Creates a dictionary to lookup what category number is what. \n",
    "# NOTE: this is based on there being 24 categories, we have 25, somehwere this will go wrong...\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxy\"\n",
    "dictionary = {}\n",
    "for i in range(24):\n",
    "    dictionary[i] = alphabet[i]\n",
    "dictionary\n",
    "\n",
    "def predict_letter(file_path):\n",
    "    '''Given an image, load it, scale for model and predict letter'''\n",
    "    show_image(file_path)\n",
    "    image = load_and_scale_image(file_path)\n",
    "    image = image_utils.img_to_array(image)\n",
    "    image = image.reshape(1,28,28,1) \n",
    "    image = image/255\n",
    "    prediction = model.predict(image)\n",
    "    print(prediction) # print the whole prediction array, probability for each category.\n",
    "    # convert prediction to letter\n",
    "    predicted_letter = dictionary[np.argmax(prediction)]\n",
    "    return predicted_letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec6d553-bfbb-4065-b03a-6c60d1832b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_letter(\"data/sign_mnist/b.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b00c374-fb33-4806-ace7-ced41c1e94c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_letter('data/sign_mnist/c.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb6ee60-37de-4988-ad67-050d66293e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_letter('data/sign_mnist/a.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9010656-d744-4cf8-9d53-63e48048c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_letter('data/sign_mnist/l.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb9e5e-4db3-4d0d-93ba-400012277d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_letter('data/sign_mnist/j.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8012ca-578f-4e6b-95f6-00bcce777b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Biology",
   "language": "python",
   "name": "aibio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import plotnine as pn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The data and problem\n",
    "\n",
    "For this lesson, we'll work with a new dataset of unknown origin (for the moment). \n",
    "\n",
    "In a not too unbelievable scenario, imagine that your boss has given you these data and said, \"I don't really know what all of this stuff means, but I need you to model this and get me a model to predict $y$ using these data!\", and of course they add, \"I need this by the end of the day!\"\n",
    "![Drawing of a boss and person at computer](images/the_boss_small.jpg)\n",
    "\n",
    "\n",
    "Let's load the data and start looking at it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/regularization.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x12</th>\n",
       "      <th>x13</th>\n",
       "      <th>x14</th>\n",
       "      <th>x15</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>x19</th>\n",
       "      <th>x20</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.126504</td>\n",
       "      <td>0.477706</td>\n",
       "      <td>0.313475</td>\n",
       "      <td>0.514386</td>\n",
       "      <td>0.743750</td>\n",
       "      <td>0.234681</td>\n",
       "      <td>0.671997</td>\n",
       "      <td>0.270976</td>\n",
       "      <td>0.519267</td>\n",
       "      <td>0.361153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571807</td>\n",
       "      <td>0.281180</td>\n",
       "      <td>0.814374</td>\n",
       "      <td>0.527172</td>\n",
       "      <td>0.550691</td>\n",
       "      <td>0.021509</td>\n",
       "      <td>0.028173</td>\n",
       "      <td>0.837283</td>\n",
       "      <td>0.261703</td>\n",
       "      <td>4.077128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.121590</td>\n",
       "      <td>0.483381</td>\n",
       "      <td>0.724387</td>\n",
       "      <td>0.818556</td>\n",
       "      <td>0.872479</td>\n",
       "      <td>0.441711</td>\n",
       "      <td>0.795640</td>\n",
       "      <td>0.474486</td>\n",
       "      <td>0.157817</td>\n",
       "      <td>0.251904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403155</td>\n",
       "      <td>0.372762</td>\n",
       "      <td>0.226465</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.089564</td>\n",
       "      <td>0.913374</td>\n",
       "      <td>0.911584</td>\n",
       "      <td>0.784988</td>\n",
       "      <td>0.035362</td>\n",
       "      <td>4.981252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.698551</td>\n",
       "      <td>0.036947</td>\n",
       "      <td>0.723721</td>\n",
       "      <td>0.459921</td>\n",
       "      <td>0.602351</td>\n",
       "      <td>0.560279</td>\n",
       "      <td>0.028867</td>\n",
       "      <td>0.414398</td>\n",
       "      <td>0.104540</td>\n",
       "      <td>0.906528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.960674</td>\n",
       "      <td>0.529909</td>\n",
       "      <td>0.911473</td>\n",
       "      <td>0.828246</td>\n",
       "      <td>0.468373</td>\n",
       "      <td>0.582552</td>\n",
       "      <td>0.527382</td>\n",
       "      <td>0.470915</td>\n",
       "      <td>0.756324</td>\n",
       "      <td>6.649698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.198011</td>\n",
       "      <td>0.724938</td>\n",
       "      <td>0.713586</td>\n",
       "      <td>0.493570</td>\n",
       "      <td>0.812774</td>\n",
       "      <td>0.665962</td>\n",
       "      <td>0.496994</td>\n",
       "      <td>0.145470</td>\n",
       "      <td>0.929903</td>\n",
       "      <td>0.299357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370109</td>\n",
       "      <td>0.862206</td>\n",
       "      <td>0.275417</td>\n",
       "      <td>0.717374</td>\n",
       "      <td>0.265629</td>\n",
       "      <td>0.952450</td>\n",
       "      <td>0.387913</td>\n",
       "      <td>0.477727</td>\n",
       "      <td>0.566269</td>\n",
       "      <td>6.034292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.160674</td>\n",
       "      <td>0.462549</td>\n",
       "      <td>0.399120</td>\n",
       "      <td>0.062895</td>\n",
       "      <td>0.933785</td>\n",
       "      <td>0.942120</td>\n",
       "      <td>0.269115</td>\n",
       "      <td>0.780801</td>\n",
       "      <td>0.452808</td>\n",
       "      <td>0.047896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.766598</td>\n",
       "      <td>0.838162</td>\n",
       "      <td>0.351654</td>\n",
       "      <td>0.207323</td>\n",
       "      <td>0.760390</td>\n",
       "      <td>0.860970</td>\n",
       "      <td>0.993952</td>\n",
       "      <td>0.417981</td>\n",
       "      <td>0.991003</td>\n",
       "      <td>7.588461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2        x3        x4        x5        x6        x7  \\\n",
       "0  0.126504  0.477706  0.313475  0.514386  0.743750  0.234681  0.671997   \n",
       "1  0.121590  0.483381  0.724387  0.818556  0.872479  0.441711  0.795640   \n",
       "2  0.698551  0.036947  0.723721  0.459921  0.602351  0.560279  0.028867   \n",
       "3  0.198011  0.724938  0.713586  0.493570  0.812774  0.665962  0.496994   \n",
       "4  0.160674  0.462549  0.399120  0.062895  0.933785  0.942120  0.269115   \n",
       "\n",
       "         x8        x9       x10  ...       x12       x13       x14       x15  \\\n",
       "0  0.270976  0.519267  0.361153  ...  0.571807  0.281180  0.814374  0.527172   \n",
       "1  0.474486  0.157817  0.251904  ...  0.403155  0.372762  0.226465  0.002351   \n",
       "2  0.414398  0.104540  0.906528  ...  0.960674  0.529909  0.911473  0.828246   \n",
       "3  0.145470  0.929903  0.299357  ...  0.370109  0.862206  0.275417  0.717374   \n",
       "4  0.780801  0.452808  0.047896  ...  0.766598  0.838162  0.351654  0.207323   \n",
       "\n",
       "        x16       x17       x18       x19       x20         y  \n",
       "0  0.550691  0.021509  0.028173  0.837283  0.261703  4.077128  \n",
       "1  0.089564  0.913374  0.911584  0.784988  0.035362  4.981252  \n",
       "2  0.468373  0.582552  0.527382  0.470915  0.756324  6.649698  \n",
       "3  0.265629  0.952450  0.387913  0.477727  0.566269  6.034292  \n",
       "4  0.760390  0.860970  0.993952  0.417981  0.991003  7.588461  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should we do with this? We have 20 $x$ variables and 50 observations of those variables. How can we build a good predictive model for these data?\n",
    "\n",
    "Where do we start?\n",
    "\n",
    "Once we're ready to start looking at models, how do we select variables?\n",
    "\n",
    "### Number of possible models\n",
    "\n",
    "As a bit of an aside, model selection quickly blows up if we want to try all possible combinations. The table below shows the number of models that would need to be evaluated for $p$ features or predictors.\n",
    "\n",
    "$p$ (number of predictors): | 1      | 2     | 10    | 20    | 40    | 100 \n",
    "--------------------------|--------|---------|-------|-------|-------|-----\n",
    "No of models for $p$ predictors| 2   |  4    | 1,024 | 1,0448,576| 1.1 X 10<sup>12</sup> | 1.3 X 10<sup>30</sup>\n",
    "Add square of each predictor | 4 | 16 | 1,048,576 | 1.1 X 10<sup>12</sup> | 1.2 X 10<sup>24</sup> | 1.6 X 10<sup>60</sup>\n",
    "Add pairwise interactions | 2 | 5 | 3.6 X 10<sup>13</sup> | 1.6 X 10<sup>57</sup> | 6.4 X 10<sup>234</sup> | ? ðŸ¤¯ \n",
    "\n",
    "**Fun fact:** Estimated number of atoms in the observable universe: ~10<sup>80</sup>\n",
    "\n",
    "So...with 20 features, we are approaching the number of atoms in the universe...\n",
    "\n",
    "Somehow need to control for the number of parameters in our model!\n",
    "\n",
    "\n",
    "## 2. Multiple linear regression\n",
    "\n",
    "Let's explore multiple linear regression first to see what happens. _Note that, in the interest of time, I am leaving out exploratory data analyses and visualizations, which is where you should start!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.9679699534659556 MSE: 0.07828974536159014\n"
     ]
    }
   ],
   "source": [
    "# Get the dataset ready\n",
    " \n",
    " \n",
    "# Create and fit a simple LinearRegression model\n",
    "\n",
    "\n",
    "def mse(model, X, y):\n",
    "    y_hat = model.predict(X)\n",
    "    err = np.mean((y - y_hat)**2)\n",
    "    return err\n",
    "\n",
    "print('R^2:', model.score(X, y), 'MSE:', mse(model, X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got a model, let's use 5-fold cross-validation to estimate how well it will work for new (i.e., non-training) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c-v R^2: 0.7734350974615244 c-v MSE: 0.38341921219406305\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('c-v R^2:', cv_r2, 'c-v MSE:', cv_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lasso regularization\n",
    "\n",
    "Remember that a model that is too complex, we tend to overfit the training data and get high variance, and if it's not complex enough, we underfit the training data and get high bias. Our goal is to find the model with just enough complexity. We can try to do this as part of the model optimization procedure.\n",
    "\n",
    "So far, we've user the $MSE$ as our loss function. And our models look something like this:\n",
    "\n",
    "$$ \\hat{y} = b_0 + b_{1}x_{1} + b_{2}x_{2} + \\ldots + b_{p}x_{p} $$\n",
    "\n",
    "Let's define:  $\\ell_1 penalty = \\lambda \\sum_{i=1}^{p} |b_{p}|$\n",
    "\n",
    "And then we can make a new loss function that is: \n",
    "\n",
    "   $$  loss = MSE +  \\lambda \\sum_{i=1}^{p} |b_{p}|$$\n",
    "\n",
    "As the coefficient for a feature increases, the $\\ell_1 penalty$ also increases. If the model can push the $b$ for a feature to 0, that feature drops out of the model. By adjusting $\\lambda$, we can adjust the effect of the $\\ell_1 penalty$.\n",
    "\n",
    "This is known as the Lasso, short for **l**east **a**bsolute **s**hrinkage and **s**election **o**perator.\n",
    "\n",
    "Okay, so let's see what the lasso ($\\ell_1$-regularized regression) does with these data.  We can use scikit-learn's `Lasso` estimator for this.  Note that scikit-learn uses `alpha` for the name of the parameter referred to as $\\lambda$ virtually everywhere else (perhaps because `lambda` is a reserved word in Python).\n",
    "\n",
    "As a general practice, you should always **make sure that your predictor variables are on the same scale when fitting a regularized regression model**.  We can have `Lasso` do this for us automatically by passing `normalize=True` when we create the `Lasso` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.          1.74023441  0.          2.55563706 -0.          0.04030546\n",
      "  0.          0.          0.          0.         -0.         -0.\n",
      " -0.          0.          0.          0.38676971 -0.          0.69308938\n",
      "  0.          3.96090209]\n",
      "c-v R^2: 0.9085724964244137 c-v MSE: 0.1856678640282654\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "res = cross_val_score(model, X, y, cv=kf, scoring=mse)\n",
    "cv_mse = res.mean()\n",
    "res = cross_val_score(model, X, y, cv=kf)\n",
    "cv_r2 = res.mean()\n",
    "print('c-v R^2:', cv_r2, 'c-v MSE:', cv_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ridge regression\n",
    "\n",
    "Another method with similar goals, but different results is known as ridge regression. This uses what is referred to as the $\\ell_1 penalty$\n",
    "\n",
    "Let's define:  $\\ell_2 penalty = \\lambda \\sum_{i=1}^{p} b_{p}^{2}$\n",
    "\n",
    "Again, we can add the $\\ell_1 penalty$ to the $MSE$.\n",
    "\n",
    "Let's see what happens if we use ridge regression ($\\ell_2$-regularized regression) with these data.  We can use scikit-learn's `Ridge` estimator to do this, and again, don't forget to make sure that your predictor variables are on the same scale.\n",
    "\n",
    "All we need to do is copy the code from above and change `Lasso` to `Ridge` in the `model` line. The $\\lambda$/`alpha` values have different effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.63682077e-05  1.00154748e-03  8.44346232e-04  2.57988981e-03\n",
      " -8.40003829e-04  1.31574521e-03  7.60368873e-05  1.67512363e-04\n",
      "  2.40974311e-04  1.33912243e-05 -5.31923545e-04 -9.82878522e-04\n",
      "  4.16529484e-04 -1.05307802e-03  1.42622322e-03  1.73305030e-03\n",
      "  2.64957843e-04  5.07204122e-04 -9.63096337e-04  3.89300983e-03]\n",
      "c-v R^2: -0.19417050653980628 c-v MSE: 2.5398869633291987\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. How does $\\lambda$ influence the parameter estimates?\n",
    "\n",
    "Before we move on, let's take a closer look at how changing lambda influences the parameter estimates for both the lasso and ridge regression.\n",
    "\n",
    "**Affect of $\\lambda$ in Lasso Regression**\n",
    "\n",
    "![Graph showing how the features drop out as lambda is increased with Lasso. 4 key features are highlighted](images/Lasso_feature_dropout.png)\n",
    "\n",
    "**Affect of $\\lambda$ in Ridge Regression**\n",
    "\n",
    "![Graph showing how the features drop out as lambda is increased with Ridge. 4 key features are highlighted](images/Ridge_feature_dropout.png)\n",
    "\n",
    "\n",
    "Lasso and ridge regression are examples of *regularization* or *shrinkage* methods, because they regularize/constrain the parameter estimates by shrinking them towards 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Choosing $\\lambda$ and a modeling procedure\n",
    "\n",
    "How should we choose the \"best\" value of $\\lambda$ for the lasso or ridge regression?  How should we decide whether to use the lasso or ridge regression?  What ideas do you have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.          1.72975135  0.          2.54682211 -0.          0.03635612\n",
      "  0.          0.          0.          0.         -0.         -0.\n",
      " -0.          0.          0.          0.38346476 -0.          0.68477174\n",
      "  0.          3.95204938]\n",
      "Lambda: 0.010312186772825561\n",
      "c-v R^2: 0.8924462779664101 c-v MSE: 0.22780219539690322\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.01741600e-02  1.70867833e+00 -8.13488095e-03  2.55428480e+00\n",
      " -6.58900869e-02  3.58797050e-01  7.29375999e-02  1.60369640e-02\n",
      "  1.96833899e-01  6.20966733e-02 -1.87935489e-01 -1.74614783e-02\n",
      "  1.10110313e-02  2.39105430e-01  5.19876052e-04  6.76011098e-01\n",
      " -1.11593813e-01  8.22592569e-01 -1.81990939e-01  3.72627101e+00]\n",
      "Lambda: 0.1\n",
      "c-v R^2: 0.7457186398711244 c-v MSE: 0.4965315463091534\n"
     ]
    }
   ],
   "source": [
    "lambdas = np.geomspace(0.1, 10, 100) # Unlike LassoCV, we need to provide the lambds, here we use a logarithmic scale.\n",
    "                                    # Note that we need to update the search range--0.001 should be good in the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Why/how do these methods work? Is one better than the other?\n",
    "\n",
    "We'll start by revealing where the data we've been modeling actually came from, which will give us some insight into how these methods work.  Then we'll dig a little deeper into how these methods are able to decrease the expected test error.\n",
    "\n",
    "These data were generated using: $y=1+ 2x_2 + 3x_4 + x_{18} + 4x_{20} + \\epsilon ;    Var[\\epsilon = 0.4] $\n",
    "\n",
    "Look back the coefficients Lasso selected. Quite amazing!\n",
    "\n",
    "### Returning to the bias-variance trade-off\n",
    "\n",
    "The graph below shows the estimated test $MSE$ for models fit to data generated with the process used to generate the data we used in red. In blue is the Bias estimate and in green the variance. The vertical dashed line is the selected $\\lambda$\n",
    "\n",
    "![Graph of the estimated test MSE for models fit to data generated with the process used to generate the data we used in red. In blue is the Bias estimate and in green the variance.](images/Lasso_Bias-Variance_trade-off.png)\n",
    " \n",
    "Notice that the model selected by Lasso does have bias! But we are trading a big reduction in variance for a small increase in bias.\n",
    "\n",
    "\n",
    "### Key points to remember:\n",
    "\n",
    "* The Lasso can drive coefficients to 0; thus, it can perform _feature selection_.\n",
    "* Ridge regression constrains coefficient values (_shrinks_ them), but does not remove them from the model.\n",
    "\n",
    "Which is best depends on your data and the goals of the analysis. In our case, most of the parameters were just noise, so Lasso performed better. If all of the parameters really played into generating the data, Ridge regression likely would have done better. **It depends on the data!**\n",
    "\n",
    "## 8. Other methods\n",
    "\n",
    "There are many other methods available for model/feature selection.  If you've had a statistics course, you've no doubt learned some of them.\n",
    "\n",
    "Unfortunately, we don't have time to cover any of these alternative methods in detail in this course.  I will mention, though, that _dimensionality reduction_ is another widely used, general approach to dealing with model complexity and certainly worth learning about.  _Principle components regression_ is one example of this approach. One downside is that the models are less interpretable.\n",
    "\n",
    "Another method I should mention is the _elastic net_, which combines $\\ell_1$ and $\\ell_2$ penalties into a single fitting procedure and can benefit from the desirable properties of both the lasso and ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data analysis lab\n",
    "\n",
    "Now, put together everything that you've learned over the last few units (prediction error, bias and variance; methods for estimating test/prediction error; regularization techniques) and apply it to a data analysis problem.  Returning to the diabetes dataset, your goal is to create a machine learning model that can accurately predict the diabetes risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "X_diabetes, y_diabetes = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UFRC Python-3.8",
   "language": "python",
   "name": "python3-3.8-ufrc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

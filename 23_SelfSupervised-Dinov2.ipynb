{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-supervised learning\n",
    "\n",
    "In the course so far, we’ve primarily encountered two dominant paradigms in machine learning: supervised and unsupervised learning. Supervised approaches have formed the backbone of much of our discussion, where models are trained on labeled datasets to learn mappings from inputs to known outputs—ranging from image classification to sequence labeling tasks. These methods rely heavily on the availability of annotated data, which can be costly or impractical to obtain at scale. On the other hand, we’ve also explored unsupervised learning techniques, which forgo labels entirely and instead aim to uncover structure or patterns inherent in the data itself. This includes clustering and dimensionality reduction. Self-supervised learning, in particular, has emerged as a powerful middle ground—leveraging the scale and flexibility of unsupervised data while achieving performance that often rivals supervised methods, without requiring manual annotation.\n",
    "\n",
    "This notebook presents a detailed walkthrough of the [DINOv2](https://arxiv.org/abs/2304.07193) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Rise of Self-Supervised Learning\n",
    "\n",
    "In recent years, self-supervised learning (SSL) has gained momentum in computer vision. With the explosion of data available online, manually labeling millions or billions of images has become prohibitively expensive. SSL leverages the inherent structure in data to learn robust representations without relying on costly annotations. Models like DINOv2 demonstrate how powerful feature representations can be learned from unlabeled data, and these features can then be adapted to a variety of downstream tasks such as classification, segmentation, and retrieval, as we will see below.\n",
    "\n",
    "![dinov2](images/dinov2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct the training corpus, the authors draw from a wide array of sources, primarily leveraging Meta's internal data lake. This includes a mix of public datasets, such as ImageNet and OpenImages, along with a significantly larger proprietary compilation referred to as LVD-142M (Large-scale Visual Data), which contains approximately 142 million images. The goal was to achieve broad coverage across different domains—ranging from natural scenes to human-made environments.\n",
    "\n",
    "Images are first filtered through a pipeline, which includes eliminating low-resolution, low-contrast, or blank images, and performing near-duplicate removal using perceptual hashing and, in some cases, CLIP-based similarity measures. The emphasis is not only on raw quantity but also on ensuring that the images retained are both meaningful and diverse in content.\n",
    "\n",
    "To prevent overrepresentation of common patterns such as human faces, animals, or heavily text-laden content, the authors implemented heuristics that favor a balanced sampling across object categories, spatial scales, and visual textures. In some configurations, CLIP embeddings are used to measure semantic diversity directly, helping filter out redundant or overly narrow image subsets. This encourages the inclusion of both coarse- and fine-grained features across different visual contexts.\n",
    "\n",
    "The final preprocessing step involves resizing and cropping the images to match the input resolution expected by Vision Transformers (224×224 for base models, up to 518×518 for larger ones). Standard data augmentation techniques like random cropping, color jitter, and Gaussian blur are used during training to create multiple views of each image. Importantly, the entire process remains self-supervised—no class labels or bounding boxes are used at any stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the Pre-trained DINOv2 Model\n",
    "\n",
    "We load the pre-trained DINOv2 model using `torch.hub`. The model is set to evaluation mode for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_name = 'dinov2_vits14_reg'  \n",
    "model = torch.hub.load('facebookresearch/dinov2', model_name)\n",
    "\n",
    "model.eval()\n",
    "print(f'Loaded {model_name} successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction with DINOv2\n",
    "\n",
    "Before applying DINOv2 to any task, we preprocess images by resizing, cropping, and normalizing them using ImageNet statistics. We then extract the dinov2 features from the preprocessed image. In the cell below, we demonstrate this using an example image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load an example image\n",
    "image = Image.open('images/American_Alligator.jpg').convert('RGB')\n",
    "input_tensor = preprocess(image).unsqueeze(0)  \n",
    "with torch.no_grad():\n",
    "    features = model(input_tensor)\n",
    "\n",
    "print('Global feature vector shape:', features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use Case: CIFAR-100 Image Classification\n",
    "\n",
    "In this section, we demonstrate how to use DINOv2 features for image classification on the CIFAR-100 (32 x 32 pixels) dataset. We load the dataset, extract features using DINOv2, and then train a simple linear classifier on top of these features. Let's start by defining our preprocessing pipeline and our dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Define transforms\n",
    "cifar_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load full datasets\n",
    "train_dataset_full = datasets.CIFAR100(root='./data', train=True, download=True, transform=cifar_transform)\n",
    "test_dataset_full = datasets.CIFAR100(root='./data', train=False, download=True, transform=cifar_transform)\n",
    "\n",
    "# Subsample\n",
    "n_train = 5000\n",
    "n_test = 1000\n",
    "train_indices = random.sample(range(len(train_dataset_full)), n_train)\n",
    "test_indices = random.sample(range(len(test_dataset_full)), n_test)\n",
    "\n",
    "train_dataset = Subset(train_dataset_full, train_indices)\n",
    "test_dataset = Subset(test_dataset_full, test_indices)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f'CIFAR-100 subset loaded: {n_train} training samples, {n_test} test samples.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define a linear probe to add to our encoder model. This will have input dimensions corresponding to the feature_dim of dinov2 and output dimensions corresponding to the CIFAR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(in_features, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Update feature_dim to match the output of your DINOv2 model (if you change the model to other architectures)\n",
    "feature_dim = 384\n",
    "num_classes = 100\n",
    "classifier = LinearClassifier(feature_dim, num_classes).to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=1e-2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def extract_features(batch):\n",
    "    with torch.no_grad():\n",
    "        feats = model(batch)\n",
    "    return feats\n",
    "    \n",
    "def train_classifier(epochs=5):\n",
    "    classifier.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        batch_count = 0\n",
    "        print(f\"Starting epoch {epoch+1}/{epochs}\")\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\", unit=\"batch\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            feats = extract_features(inputs)\n",
    "            outputs = classifier(feats)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            batch_count += 1\n",
    "        avg_loss = running_loss / batch_count\n",
    "        print(f\"Epoch {epoch+1} complete, Average Loss: {avg_loss:.4f}\")\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "def evaluate_classifier():\n",
    "    classifier.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print(\"Evaluating classifier...\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            feats = extract_features(inputs)\n",
    "            outputs = classifier(feats)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Evaluation complete. Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classifier(epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Use Case: Unsupervised Segmentation with DINOv2 Features\n",
    "\n",
    "DINOv2 not only produces strong global representations but also learns spatially rich features. In this section, we demonstrate a simple unsupervised segmentation approach. \n",
    "\n",
    "We use the model's intermediate method (`forward_features`) that outputs patch tokens. These tokens (excluding any class token) are reshaped into a grid corresponding to image patches. We then perform k-means clustering on these patch features to segment the image into regions with similar characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def extract_patch_features(image_tensor):\n",
    "    with torch.no_grad():\n",
    "        out = model.forward_features(image_tensor)\n",
    "    if isinstance(out, dict):\n",
    "        if 'x_norm_patchtokens' in out:\n",
    "            patch_tokens = out['x_norm_patchtokens']\n",
    "        else:\n",
    "            raise ValueError(\"Expected 'x_norm_patchtokens' key not found in model output.\")\n",
    "    else:\n",
    "        patch_tokens = out\n",
    "\n",
    "    num_tokens = patch_tokens.shape[1]\n",
    "    grid_size = int(np.sqrt(num_tokens))\n",
    "    patch_tokens = patch_tokens.reshape(grid_size * grid_size, -1)\n",
    "    return patch_tokens, grid_size\n",
    "\n",
    "def segment_image(image_tensor, n_clusters=6):\n",
    "    patch_feats, grid_size = extract_patch_features(image_tensor)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(patch_feats.cpu().numpy())\n",
    "    labels = kmeans.labels_.reshape(grid_size, grid_size)\n",
    "    return labels\n",
    "\n",
    "# Load the same example image\n",
    "image = Image.open('images/American_Alligator.jpg').convert('RGB')\n",
    "input_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "seg_labels = segment_image(input_tensor, n_clusters=4)\n",
    "    \n",
    "plt.figure(figsize=(12,6))\n",
    "    \n",
    "# Display original image\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(image)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "    \n",
    "# Display segmentation mask\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(seg_labels, cmap='viridis')\n",
    "plt.title('Segmentation Mask')\n",
    "plt.axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try without clustering, but illustrating features using a PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def visualize_pca_features(image_tensor):\n",
    "    patch_tokens, grid_size = extract_patch_features(image_tensor)\n",
    "    # Perform PCA to reduce features to 3 dimensions\n",
    "    pca = PCA(n_components=3)\n",
    "    pca_features = pca.fit_transform(patch_tokens.cpu().numpy())\n",
    "    # Normalize each channel to the [0,1] range for visualization\n",
    "    pca_features = (pca_features - pca_features.min(axis=0)) / (pca_features.max(axis=0) - pca_features.min(axis=0) + 1e-8)\n",
    "    # Reshape to the grid dimensions (H, W, 3)\n",
    "    pca_image = pca_features.reshape(grid_size, grid_size, 3)\n",
    "    return pca_image\n",
    "\n",
    "# Load the same example image and visualize PCA features alongside the original image\n",
    "image = Image.open('images/American_Alligator.jpg').convert('RGB')\n",
    "input_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "pca_image = visualize_pca_features(input_tensor)\n",
    "    \n",
    "plt.figure(figsize=(12,6))\n",
    "# Original image\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(image)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "    \n",
    "# PCA feature visualization\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(pca_image)\n",
    "plt.title('PCA Visualization of Patch Features')\n",
    "plt.axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Use Case: Image Retrieval using DINOv2 Features\n",
    "\n",
    "Another powerful application of DINOv2 is image retrieval. In this demonstration, we compute global features for a subset of images (using CIFAR-100 as an example) and, given a query image, retrieve the most similar images based on cosine similarity of their feature vectors. Let's start by extracting the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "def extract_global_feature(image_tensor):\n",
    "    with torch.no_grad():\n",
    "        feat = model(image_tensor)\n",
    "    return feat\n",
    "    \n",
    "def unnormalize(img, mean, std):\n",
    "    for t, m, s in zip(img, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return img\n",
    "    \n",
    "def compute_dataset_features(dataset, subset_size=100):\n",
    "    indices = list(range(subset_size))\n",
    "    subset = Subset(dataset, indices)\n",
    "    loader = DataLoader(subset, batch_size=32, shuffle=False)\n",
    "    features_list = []\n",
    "    images_list = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            feats = model(imgs)\n",
    "            features_list.append(feats.cpu())\n",
    "            images_list.extend([img for img in imgs.cpu()])\n",
    "    features = torch.cat(features_list, dim=0)\n",
    "    return features, images_list\n",
    "\n",
    "# Compute features for a subset of CIFAR-100 test images\n",
    "dataset_features, dataset_images = compute_dataset_features(test_dataset, subset_size=1000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's retrieve the top5 most similar images and plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar(query_tensor, dataset_features, top_k=5):\n",
    "    query_feat = extract_global_feature(query_tensor.to(device)).cpu()\n",
    "    query_norm = query_feat / query_feat.norm(dim=1, keepdim=True)\n",
    "    dataset_norm = dataset_features / dataset_features.norm(dim=1, keepdim=True)\n",
    "    similarities = (dataset_norm @ query_norm.t()).squeeze()\n",
    "    top_indices = similarities.topk(top_k).indices\n",
    "    return top_indices, similarities[top_indices]\n",
    "    \n",
    "# Select a random query image from the test set\n",
    "import random\n",
    "query_idx = random.randint(0, len(dataset_images)-1)\n",
    "query_image = dataset_images[query_idx]\n",
    "\n",
    "indices, sims = retrieve_similar(query_image.unsqueeze(0), dataset_features, top_k=5)\n",
    "\n",
    "mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.subplot(1,6,1)\n",
    "img = unnormalize(query_image.clone(), mean, std)\n",
    "plt.imshow(img.permute(1,2,0).clamp(0,1))\n",
    "plt.title('Query')\n",
    "plt.axis('off')\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    plt.subplot(1,6,i+2)\n",
    "    img = unnormalize(dataset_images[idx].clone(), mean, std)\n",
    "    plt.imshow(img.permute(1,2,0).clamp(0,1))\n",
    "    plt.title(f'Sim {i+1}')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Zero-shot Depth Estimation\n",
    "\n",
    "In this example, we generate a depth-like map in a zero-shot, unsupervised manner using DINOv2 features. \n",
    "We extract patch tokens from a pretrained DINOv2 model, apply PCA to project them into a lower-dimensional \n",
    "space, and visualize the first few principal components as a proxy for relative depth. This method captures \n",
    "coarse scene geometry without any explicit depth supervision or fine-tuning, leveraging the strong inductive \n",
    "biases encoded in DINOv2's learned representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "# Load and preprocess image\n",
    "torch.manual_seed(32)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "image = Image.open('images/American_Alligator.jpg').convert('RGB')\n",
    "input_tensor = preprocess(image).unsqueeze(0).to(device)  # (1, 3, 224, 224)\n",
    "\n",
    "\n",
    "# Extract patch tokens\n",
    "with torch.no_grad():\n",
    "    features = model.forward_features(input_tensor)\n",
    "    patch_tokens = features['x_norm_patchtokens'].squeeze(0).cpu().numpy()  # (N, D)\n",
    "\n",
    "# PCA on patch tokens\n",
    "k = 3  # number of PCs to combine\n",
    "\n",
    "pca = PCA(n_components=k)\n",
    "pc_feats = pca.fit_transform(patch_tokens)  # shape: (N, 3)\n",
    "\n",
    "# Determine grid size\n",
    "N = patch_tokens.shape[0]\n",
    "grid_size = int(math.sqrt(N))\n",
    "assert grid_size * grid_size == N, \"Patch tokens do not form a square grid\"\n",
    "\n",
    "# Visualization 2: Depth-like map\n",
    "depth_map = pc_feats[:, :k].mean(axis=1)\n",
    "depth_map = depth_map.reshape(grid_size, grid_size)\n",
    "depth_map -= depth_map.min()\n",
    "depth_map /= depth_map.max()\n",
    "\n",
    "# Upsample both to 224x224\n",
    "\n",
    "depth_map_up = F.interpolate(torch.tensor(depth_map).unsqueeze(0).unsqueeze(0),\n",
    "                             size=(224,224), mode='bilinear', align_corners=False).squeeze().numpy()\n",
    "depth_map_up = 1.0 - depth_map_up\n",
    "\n",
    "# Original image (unnormalized)\n",
    "image_vis = transforms.ToTensor()(image).permute(1, 2, 0).numpy()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image_vis)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(depth_map_up, cmap='inferno')\n",
    "plt.title(\"Depth-like Map\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Zero-shot Semantic Correspondence\n",
    "\n",
    "This section demonstrates how DINOv2 patch embeddings can be used for semantic correspondence between two related images \n",
    "in a fully zero-shot setting. We extract patch-level features from each image, then compute dense pairwise cosine similarity \n",
    "to find the most similar patch in the second image for every patch in the first. This produces a coarse but meaningful mapping \n",
    "between semantically aligned regions—without supervision, labels, or fine-tuning. The resulting correspondences reveal how \n",
    "self-supervised DINOv2 features encode spatial semantics and object part alignment across views or instances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = Image.open('images/cat1.jpg').convert('RGB')\n",
    "img2 = Image.open('images/cat2.jpg').convert('RGB')\n",
    "\n",
    "x1 = preprocess(img1).unsqueeze(0).to(device)\n",
    "x2 = preprocess(img2).unsqueeze(0).to(device)\n",
    "\n",
    "# --- Extract patch features ---\n",
    "with torch.no_grad():\n",
    "    f1 = model.forward_features(x1)['x_norm_patchtokens'][0]  # [N, C]\n",
    "    f2 = model.forward_features(x2)['x_norm_patchtokens'][0]  # [N, C]\n",
    "\n",
    "# --- Match patches via cosine similarity ---\n",
    "sim = torch.nn.functional.cosine_similarity(f1[:, None, :], f2[None, :, :], dim=-1)  # [N, N]\n",
    "matches = sim.argmax(dim=1)  # [N]\n",
    "\n",
    "# --- Convert patch indices to 2D coordinates ---\n",
    "def patch_coords(n_patches, H, W, device):\n",
    "    grid_h = int(round(n_patches ** 0.5))\n",
    "    grid_w = grid_h  # assume square\n",
    "    assert grid_h * grid_w == n_patches, f\"Expected square patch grid, got {n_patches} patches.\"\n",
    "\n",
    "    y = (torch.arange(grid_h, device=device) + 0.5) * (H / grid_h)\n",
    "    x = (torch.arange(grid_w, device=device) + 0.5) * (W / grid_w)\n",
    "    xx, yy = torch.meshgrid(x, y, indexing='xy')\n",
    "    coords = torch.stack([xx.flatten(), yy.flatten()], dim=1)\n",
    "    return coords\n",
    "\n",
    "H, W = x1.shape[-2:]\n",
    "coords1 = patch_coords(f1.shape[0], H, W, device)\n",
    "coords2 = patch_coords(f2.shape[0], H, W, device)\n",
    "\n",
    "scores = sim.max(dim=1).values  # [N]\n",
    "mask = scores > 0.75  # tune threshold\n",
    "coords1_filtered = coords1[mask]\n",
    "coords2_filtered = coords2[matches][mask]\n",
    "\n",
    "matched_coords1 = coords1_filtered\n",
    "matched_coords2 = coords2_filtered\n",
    "\n",
    "# --- Visualization (CPU + NumPy) ---\n",
    "matched_coords1 = matched_coords1.cpu().numpy()\n",
    "matched_coords2 = matched_coords2.cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "combined_img = np.hstack([np.array(img1.resize((224, 224))), np.array(img2.resize((224, 224)))])\n",
    "ax.imshow(combined_img)\n",
    "\n",
    "for pt1, pt2 in zip(matched_coords1, matched_coords2):\n",
    "    x1, y1 = pt1\n",
    "    x2, y2 = pt2\n",
    "    ax.plot([x1, x2 + 224], [y1, y2], color='cyan', linewidth=1.5, alpha=0.5)\n",
    "\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Biology",
   "language": "python",
   "name": "aibio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

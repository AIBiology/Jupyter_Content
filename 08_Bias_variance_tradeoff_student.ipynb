{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Trade-off\n",
    "\n",
    "[Last time](07_Intro_sklearn_student.ipynb), we started looking at linear regression and quickly touched on a few additional ML methods. We also briefly touched on the idea of splitting data into training and testing datasets. In this notebook, we'll explore this a bit more and address the bias-variance trade-off and model complexity.\n",
    "\n",
    "We will grab a new dataset, the built in `diabetes` dataset and explore this using linear regression. Here's the information on that dataset:\n",
    "\n",
    "The dataset for this example is another one built into `sklearn` and appears to originate from Efron *et al.* ([2003](https://hastie.su.domains/Papers/LARS/LeastAngle_2002.pdf)) and has 442 patients measured for 10 baseline variables and a response variable that measures disease progression one-year after baseline.\n",
    "\n",
    "> Ten baseline variables, age, sex, body mass index, average blood pressure, and six blood serum measurements were obtained for each of n = 442 diabetes patients, as well as the response of interest, a quantitative measure of disease progression one year after baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    " # Load it first as a dataframe to take a quick look at the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Reload for sklearn use"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first start, let's do  a simple linear regression using blood pressure score (bp,column 3) and our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks plausibly linear...right?? 🧐\n",
    "\n",
    "Let's give it a quick shot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X_bp,y_diabetes)\n",
    "\n",
    "def check_model(model, x, y):\n",
    "    '''A function to get R^2 and plot regression'''\n",
    "    \n",
    "    Xfit = np.linspace(-0.15,0.15, len(y))[:, None] # Get some X-values for the fit curve\n",
    "    yfit =  model.predict(Xfit) # Get the predicted y-values\n",
    "\n",
    "    # Print model score R^2\n",
    "    print(f'R^2: {model.score(x,y)}') # Get R^2 using the original data\n",
    "    \n",
    "    # Print MSE for the model\n",
    "    print(f'MSE: {np.mean((y-model.predict(x))**2)}')\n",
    "          \n",
    "    # Visilize the results\n",
    "    plt.scatter(x,y) # Scatter plot of our training data\n",
    "    plt.plot(Xfit,yfit) # Our linear model\n",
    "    \n",
    "check_model(model, X_bp, y_diabetes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines and Polynomial features\n",
    "\n",
    "That doesn't look too bad and more or less corresponds to what I was expecting. Our $R^2$, a measure of how much of the variation in the data is explained by the model, is only 0.19 though, so not the best...\n",
    "\n",
    "One method to improve this is to add polynomial features--add the square of bp, the cube, etc. Let's explore that. We could make a dataframe manually that has bp and bp^2, but sklearn can do this for us. That is part of the `Pipeline` module.\n",
    "\n",
    "We call `Pipeline()` and pass in a list of the objects we want included in the modeling pipeline.  Each pipeline component object needs to be placed in a tuple that includes a name for the pipeline step; e.g., `('name', pipeline_component)`.  After we've created our `Pipeline`, we can use it just like any model object in scikit-learn.  For example, we can call `fit()`, `predict()`, `score()`, and so on.\n",
    "\n",
    "Note that, when using `LinearRegression` with `PolynomialFeatures`, you need to instantiate your `LinearRegression` object with the argument `fit_intercept=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our $R^2$ went up and out MSE went down!\n",
    "\n",
    "Let's see if even higher order polynomials work even better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we keep adding higher order polynomials, $R^2$ does keep improving and MSE keep going down. For these data, we're not getting great fit, but hopefully you are starting to see a pattern here...the more complex the model, the better it will fit the data! But...remember we are using **all** the data and the more complex models are essentially fitting those data and will likely do a poor job of fitting new data that the model has not seen.\n",
    "\n",
    "## The Bias-Variance Trade-off\n",
    "\n",
    "Let's return to an example from the text, reworked a bit to fit with our `Pipeline` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return Pipeline([\n",
    "    ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
    "    ('poly', PolynomialFeatures(degree=degree)),\n",
    "    ('linreg', LinearRegression(fit_intercept=False))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(N, err=1.0, rseed=1):\n",
    "    # randomly sample the data\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    X = rng.rand(N, 1) ** 2\n",
    "    y = 10 - 1. / (X.ravel() + 0.1)\n",
    "    if err > 0:\n",
    "        y += err * rng.randn(N)\n",
    "    return X, y\n",
    "\n",
    "X, y = make_data(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n",
    "\n",
    "plt.scatter(X.ravel(), y, color='black')\n",
    "axis = plt.axis()\n",
    "for degree in [1, 2, 3, 20]:\n",
    "    model=PolynomialRegression(degree)\n",
    "    y_test = model.fit(X, y).predict(X_test)\n",
    "    plt.plot(X_test.ravel(), y_test, label=f'degree={degree}; R^2={model.score(X,y)}')\n",
    "plt.xlim(-0.1, 1.0)\n",
    "plt.ylim(-2, 12)\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, as with the bp data, as we increase the polynomial order, the $R^2$ keeps going up. But hopefully it's clear that the 20th degree polynomial is fitting the training data and will not generalize well.\n",
    "\n",
    "Quoting from the [text](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html#The-Bias-variance-trade-off):\n",
    "> the straight-line model will never be able to describe this dataset well. Such a model is said to *underfit* the data: that is, it does not have enough model flexibility to suitably account for all the features in the data; another way of saying this is that the model has high *bias*.\n",
    "\n",
    "Similarly:\n",
    "> The [high-order polynomial] model fit has enough flexibility to nearly perfectly account for the fine features in the data, but even though it very accurately describes the training data, its precise form seems to be more reflective of the particular noise properties of the data rather than the intrinsic properties of whatever process generated that data. Such a model is said to *overfit* the data: that is, it has so much model flexibility that the model ends up accounting for random errors as well as the underlying data distribution; another way of saying this is that the model has high *variance*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scored the models using $R^2$. As the text notes:\n",
    "\n",
    "> * For high-bias models, the performance of the model on the validation set is similar to the performance on the training set.\n",
    "> * For high-variance models, the performance of the model on the validation set is far worse than the performance on the training set.\n",
    "\n",
    "This image from the text provides a great illustration of the bias-variance trade-off:\n",
    "\n",
    "![PDSH Image showing bias-variance trade-off](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.03-validation-curve.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again from the text:\n",
    "> The diagram shown here is often called a validation curve, and we see the following essential features:\n",
    ">\n",
    "> * The training score is everywhere higher than the validation score. This is generally the case: the model will be a better fit to data it has seen than to data it has not seen.\n",
    "> * For very low model complexity (a high-bias model), the training data is under-fit, which means that the model is a poor predictor both for the training data and for any previously unseen data.\n",
    "> * For very high model complexity (a high-variance model), the training data is over-fit, which means that the model predicts the training data very well, but fails for any previously unseen data.\n",
    "> * For some intermediate value, the validation curve has a maximum. This level of complexity indicates a suitable trade-off between bias and variance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example that Brian Stucky introduced to help think about this is below.\n",
    "\n",
    "Imagine the following table of data:\n",
    "\n",
    "Name     | Class     | Can fly\n",
    "---------|-----------|--------\n",
    "Pileated woodpecker | Birds | Yes\n",
    "Emu | Birds | No\n",
    "Northern cardinal | Birds | Yes\n",
    "Blacktip Shark | Cartilaginous fishes | No\n",
    "Bluntnose stingray | Cartilaginous fishes | No\n",
    "Black drum | Bony fishes | No\n",
    "Florida carpenter ant | Insects | No\n",
    "Periodical cicada | Insects | Yes\n",
    "Luna moth | Insects | Yes\n",
    "\n",
    "**Our task:** Develop a rule based model to predict whether an animal can fly.\n",
    "\n",
    "**Model 1:** \n",
    "* If the animal is a bird or an insect, predict it can fly\n",
    "* Otherwise, predict that it cannot fly\n",
    "* **Accuracy: 0.78**\n",
    "\n",
    "**Model 2:**\n",
    "* If the species is a bird with a one-word name, predict that it cannot fly\n",
    "* If it is a bird with a two-word name, predict that it can fly\n",
    "* If it is an insect with a three-word name, predict that it cannot fly\n",
    "* If it is an insect with a two-word name, predict that it can fly\n",
    "* Otherwise, predict that it cannot fly\n",
    "* **Accuracy: 1.00**\n",
    "\n",
    "Which is a better model??\n",
    "\n",
    "ok....Coming back to the text and having a look at the `validation_curve`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "degree = np.arange(0, 21)\n",
    "train_score, val_score = validation_curve(estimator = PolynomialRegression(), X = X,y= y,\n",
    "                                          param_name ='poly__degree', param_range = degree, cv=7)\n",
    "\n",
    "plt.plot(degree, np.median(train_score, 1), color='blue', label='training score')\n",
    "plt.plot(degree, np.median(val_score, 1), color='red', label='validation score')\n",
    "plt.legend(loc='best')\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('score');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Test Error = Bias + Variance + Error \n",
    "\n",
    "Let's take a look at some [slides](https://docs.google.com/presentation/d/1V6F2ZcYfosgc0HD8HF3aVkkIxGkAuNFa1dUGPMLNIro/edit?usp=sharing) that help reinforce the idea below that the total testing error = bias + variance + process error:\n",
    "\n",
    "![Image showing that total testing error = bias + variance + process error](images/test_error_formula.png)\n",
    "\n",
    "\n",
    "## The full diabetes dataset\n",
    "\n",
    "Let's have a look at the full dataset and see if a model with all the features can help us out with this while still using linear regression.\n",
    "\n",
    "First we'll relead the dataset, in case we lost it along the way in this notebook...\n",
    "\n",
    "I'll note that in this case, the data are already scaled the similar sales. If they weren't, we would want to rescale things, which can be easily done in our `Pipeline` by adding a scaler: `('scaler', StandardScaler()),` at the start of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reimport the data, jsut to be sure...\n",
    "X_diabetes, y_diabetes = load_diabetes(return_X_y=True)\n",
    "\n",
    "# Let's split the data into training/testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup out model\n",
    "\n",
    "\n",
    "print(f'Training data score: {model.score(X_train,y_train)}')\n",
    "print(f'Testing data score: {model.score(X_test,y_test)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross valiadation\n",
    "\n",
    "Above we looked at splitting the full dataset into training and testing sets. By default `sklearn`'s `train_test_split` will use 0.25, or train on 75% of the data and test on 25% of the data.\n",
    "\n",
    "For the diabetes data, that probably isn't a huge deal since there's 442 samples. Because of that, and because we weren't getting the best results with this model anyway, let's load the data that were generated for the [slides](https://docs.google.com/presentation/d/1V6F2ZcYfosgc0HD8HF3aVkkIxGkAuNFa1dUGPMLNIro/edit?usp=sharing). It doesn't really matter for this, but just to throw it out there, the formula used to generate the data was:\n",
    "\n",
    "$$ y = \\frac{1}{2} \\cos(3\\pi x) + \\frac{3\\pi x}{5} + \\epsilon ,$$\n",
    "\n",
    "where $Var[\\epsilon] = 0.5$.\n",
    "\n",
    "The file at `data/weird_function_data.csv` has 100 data points generated using this function. We'll load that and split the data, using 50% for training and 50% for testing. Then we'll fit models with different numbers of polynomial features, estimate and plot the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotnine as pn\n",
    "\n",
    "\n",
    " # Get the x values\n",
    " # get the y values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split our data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "def mse(model, x, y):\n",
    "    return np.mean((y-model.predict(x))**2)\n",
    "    \n",
    "err = []\n",
    "degrees = range(15)\n",
    "\n",
    "# Loop over degrees\n",
    "\n",
    "\n",
    "\n",
    "err_df = pd.DataFrame({'degree':degrees, 'mse':err})\n",
    "# Line plot\n",
    "plt.plot(err_df['degree'], err_df['mse'], label='MSE by Degree')\n",
    "\n",
    "# Overlay points\n",
    "plt.scatter(err_df['degree'], err_df['mse'])\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('MSE')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "                                              "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above code each time...you should get quite different results with each realization. Each run splits the data into training and testing datasets randomly. With each split, we get quite different values for out error estimates.\n",
    "\n",
    "Especially with smaller datasets, we can't just run a single realization and pick a number of parameters with the lowest error! \n",
    "\n",
    "The variance is related to the dataset size, so this is more pronounced with smaller datasets. Making this worse, we have 100 data points, but are only using 50 to estimate the model.\n",
    "\n",
    "Depending on the data, we're potentially losing a lot of data here by fitting our model on only half the data.\n",
    "\n",
    "### $k$-fold Cross-validation\n",
    "\n",
    "_$k$-fold cross-validation_ is a technique that overcomes some of the problems with using a single train/validation split.  The idea is that we randomly split our dataset into $k$ (approximately) equal-sized subsets.  We use each subset as the validation set in turn, yielding a total of $k$ estimates of our test error, which we then average to obtain our final test error estimate.  (Note that the way I am using our custom $MSE$ function in `cross_val_score()` is a little bit of a hack.  See the [scki-kit learn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) for the approved way of doing this.)\n",
    "\n",
    "Here's the image from the text on this:\n",
    "\n",
    "![PDSH diagram illustrating k-fold cross-validation](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.03-5-fold-CV.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "model = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=5)),\n",
    "    ('linreg', LinearRegression(fit_intercept=False))\n",
    "])\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "res = cross_val_score(model, X, y, cv=kf, scoring=mse)\n",
    "    \n",
    "print(res)\n",
    "print(f'Average MSE: {res.mean()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run this for all the degrees like we did above for the simple train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "err = []\n",
    "degrees = range(15)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "for degree in degrees:\n",
    "    # Make our model\n",
    "    model = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree)),\n",
    "        ('linreg', LinearRegression(fit_intercept=False))\n",
    "    ])\n",
    "\n",
    "    # Fit the model\n",
    "    res = cross_val_score(model, X, y, cv=kf, scoring=mse)\n",
    "    err.append(res.mean()) # Get MSE for model and append to list\n",
    "\n",
    "err_df = pd.DataFrame({'degree':degrees, 'mse':err})\n",
    "# Line plot\n",
    "plt.plot(err_df['degree'], err_df['mse'], label='MSE by Degree')\n",
    "\n",
    "# Overlay points\n",
    "plt.scatter(err_df['degree'], err_df['mse'])\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('MSE')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "                       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we run this multiple times, the shape should stay more similar, and we see that the 5th degree polynomial generally has the lowest error.\n",
    "\n",
    "The extreme case of $k$-fold cross-validation is where the number of folds, $k$, is equal to the number of data points. In that case, we train on all but one in each fold, cycling through each data point. This is refereed to as **leave-one-out cross-validation**.\n",
    "\n",
    "\n",
    "## Grid Search to select best model hyperparameters\n",
    "\n",
    "The polynomial degree, is one hyperparameter we can use in building our model. So far, we've left other hyperparameters as their defaults. We could systematically work through all the different combinations, but even when this is practical, it is tedious!\n",
    "\n",
    "`sklearn` provides a handy `grid_search` to help automate this, at least in cases where it is practical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'poly__degree': np.arange(21),\n",
    "              'linreg__fit_intercept': [True, False],\n",
    "              'scaler__with_mean': [True, False],\n",
    "              'scaler__with_std': [True, False]}\n",
    "\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we call the .fit method all the models will be fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best parameters are stored in grid.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then use the model using grid.best_estimator_\n",
    "\n",
    "\n",
    "# Predict our y's using the model\n",
    "y_test = model.fit(X,y).predict(X_test)\n",
    "\n",
    "\n",
    "# Plot the data in blue and the predicted data in red.\n",
    "plt.scatter(X,y) # Scatter plot of our training data\n",
    "plt.scatter(X_test,y_test, color='red') # Our linear model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "UFRC Python-3.8",
   "language": "python",
   "name": "python3-3.8-ufrc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

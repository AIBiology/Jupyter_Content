{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aad6ab19-5164-4969-b6eb-11531c45abb5",
   "metadata": {},
   "source": [
    "# Introduction to Convolutional Neural Networks, part 1\n",
    "\n",
    "We will start looking at image classification, the task of training a model to classify an image into one of a pre-set number of categories. The image below shows common computer vision tasks and is from [Shivani Kolungade](https://medium.com/@kolungade.s/object-detection-image-classification-and-semantic-segmentation-using-aws-sagemaker-e1f768c8f57d)\n",
    "\n",
    "![Image classification, object detection and segmentation by Shivani Kolungade](images/classification_detection_segmentation.png)\n",
    "\n",
    "\n",
    "## Image classification of an American Sign Language Dataset\n",
    "\n",
    "For this exercise, we will explore image classification, starting with a \"simple\" neural network and then adding convolutional layers, data augmentation and more to build better models.\n",
    "\n",
    "This notebook is inspired by the Nvidia [Fundamentals of Deep Learning](https://www.nvidia.com/en-us/training/instructor-led-workshops/fundamentals-of-deep-learning/) (instructor led)/ [Getting Started with Deep Learning](https://courses.nvidia.com/courses/course-v1:DLI+S-FX-01+V1/about) (online, asynchronous) exercises. \n",
    "\n",
    "## American Sign Language Dataset\n",
    "\n",
    "The American Sign Language [manual alphabet](https://en.wikipedia.org/wiki/American_manual_alphabet) has signs for each letter. We will skip J and Z because those signs require motion. \n",
    "\n",
    "The dataset we'll use is from [Kaggle](https://www.kaggle.com/arjaiswal/sign-mnist-using-cnn/data). There should be 24 classes...but for some reason, there are 25...We'll stick with the Kaggle data, but keep this in mind...\n",
    "\n",
    "We'll start with this dataset because it helps with the transition from the tabular data we've been working with to image data. Images are a grid of pixels, where each pixel has a brightness. For greyscale images, there is one grid, for color images, there are typically three grids, for the red, green and blue colors (RGB), often called color channels.\n",
    "\n",
    "Let's start by loading the data and examining some images.\n",
    "\n",
    "## Load the data and examine some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f7283-df18-44a7-a7c8-d302ea4bf742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from helpers_plot_history import plot_history # A function to plot training history. \n",
    "                            # We 1st used the code in 15_neural_networks.ipynb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d970832-2721-4a1d-a7c5-38463c740210",
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_train = pd.read_csv(\"data/sign_mnist/sign_mnist_train.csv\")\n",
    "sign_test = pd.read_csv(\"data/sign_mnist/sign_mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c611958a-b1c8-4985-a700-e6796e9e72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5299a3-8f71-41db-95f0-b887f2332cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = sign_train['label']\n",
    "X_train = sign_train.drop(columns='label').values\n",
    "\n",
    "y_test = sign_test['label']\n",
    "X_test = sign_test.drop(columns='label').values\n",
    "\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15551a19-09a4-43f9-98ae-55d1337e2516",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,40))\n",
    "\n",
    "num_images = 20\n",
    "for i in range(num_images):\n",
    "    row = X_train[i]\n",
    "    label = y_train[i]\n",
    "    \n",
    "    image = row.reshape(28,28)  # Note that we reshape the row into a 28X28 pixel image\n",
    "    plt.subplot(1, num_images, i+1)\n",
    "    plt.title(label, fontdict={'fontsize': 30})\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03dd1da-5bd6-4f04-8124-d8f4e0fb3def",
   "metadata": {},
   "source": [
    "## Normalize the data\n",
    "\n",
    "As we mentioned in previous classes, networks train better with standardized or normalized data. Normalization tends to work best with images, so can look at the min and max of our dataset to get those and divide by the max value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062a132-60e3-401a-9514-63eaf9519027",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.min(), X_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ce62a-c8ea-46d9-8023-b27e18e5aaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize our data (get values between 0-1)\n",
    "\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a634d683-d002-48ea-8ca0-bd2b3def5a53",
   "metadata": {},
   "source": [
    "## Labels to categorical\n",
    "\n",
    "We want out labels to be categorical. As they are now, the labels are 0, 1,...,24 (to be honest, I am not sure why there are 25 classes. Should only be 24...). That would imply label 1 is more similar to label 2 than to label 23 and that label 3 is less than label 4. Converting to categorical labels converts these to named categories, removing the order/associations among the names.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46986cb-a71f-4f30-85dd-73171afc13ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our classes to categorical\n",
    "\n",
    "num_classes = 25 # Not entirely sure what the 25th category is...\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b934eb-ed00-4d42-89cb-9deba47e35a2",
   "metadata": {},
   "source": [
    "## Make our model\n",
    "\n",
    "I've kept the code format here as it was in the Nvidia exercise. This is another common method of making a model. So far, I've used the format below to keep things similar to `sklearn Pipelines`, but I want to expose you to this `model.add` format too. The code in the block below is the same as this:\n",
    "\n",
    "    model = Sequential([\n",
    "        layers.Dense(units = 512, activation='relu', input_shape=(784,))),\n",
    "        layers.Dense(units = 512, activation='relu')),\n",
    "        layers.Dense(units = num_classes, activation='softmax'))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6175207-a26f-4c9e-a1b3-08c37b5322a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units = 512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dense(units = 512, activation='relu'))\n",
    "model.add(Dense(units = num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76321fd-ffda-47f9-9391-85b73e0e3246",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f64f65-90ed-4182-afe4-26d94a544a37",
   "metadata": {},
   "source": [
    "## Compile and fit our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7740dac1-a3eb-4e44-9703-05df91c24715",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a10ffd-40dd-487e-9dfa-8b9cfa642ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9cc2eb-7737-4710-8112-01be4d9517e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the plot_history function we made and imported from helpers_plot_history.py\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2013534a-5b93-4246-9e6e-ba024b3a0089",
   "metadata": {},
   "source": [
    "## How can we improve our model?\n",
    "\n",
    "Honestly, that's not too bad with the data we have. About 80% accuracy for greyscale images 28X28 pixels seems remarkable to me! But, notice that the test accuracy is well below the training accuracy and test loss is going up or staying the same. We are overfitting our data.\n",
    "\n",
    "We can do a number of things to improve on these results! Let's take a look.\n",
    "\n",
    "Let's look at some slides that cover convolutional kernels, pooling, dropout and data augmentation for image classification in neural networks: [Lect_06_CNNs slides](https://docs.google.com/presentation/d/1uSk7xHWZ9H6YihUP4OdHpIVws_2py_HBfbby7GpZDCA/edit?usp=sharing)\n",
    "\n",
    "Then, we can move to [part 2](17_CNNs_part2.ipynb) of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cffb72b-2b1f-46e1-a27d-d3d94874845c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.6.0",
   "language": "python",
   "name": "tensorflow-2.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

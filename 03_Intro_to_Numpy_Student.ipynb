{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Check the Kernel you are using:</b> Before we get started, if you are running this on HiPerGator, double check the kernel in use. This is shown in the top right of the window and should look like: <img src=\"images/kernel.python310.png\" alt\"Image showing that the notebook is using the Python 3.10 Full kernel\" style=\"float:right\">\n",
    "</div>\n",
    "\n",
    "# NumPy and inclusive communities\n",
    "\n",
    "[NumPy](https://numpy.org/) is undoubtedly an important package for Python and its developers (mostly volunteer) have provided a great service to the community, not only with NumPy itself, but enabling development of packages that use NumPy under the hood to add even more functionality. The developers have however done a great disservice  in failing to address issues of inclusion of diverse talents. \n",
    "\n",
    "On September, 16, 2020, *Nature* published the paper [Array Programming with NumPy](https://www.nature.com/articles/s41586-020-2649-2?amp%3Bcode=573df4db-16bd-47ad-b138-d0d9c14134f1) with 26 authors. **All** 26 authors are male! There have been many excuses offered, and commitments to improve ([NumPy Diversity and Inclusion Statement](https://numpy.org/diversity_sep2020/)).\n",
    "\n",
    "This is not news however, a [2018 analysis by Anthony Scopatz](https://nbviewer.jupyter.org/github/scopatz/nf-project-inequality/blob/9b83df3090c9b9b1b953d2905d428b71165ce607/nf-project-inequality.ipynb), found huge \"Inequality of underrepresented groups in PyData Leadership\" (this is an interesting read on its own and is presented as a Jupyter Notebook). Here's the main figure from Anthony's analysis:\n",
    "\n",
    "![image from Anthony Scopatz's analysis, linked from Reshama Shaikh's article on \"Why Women Are Flourishing In R Community But Lagging In Python\"](https://reshamas.github.io/assets/images/numfocus_os.png)\n",
    "\n",
    "The analysis showed high inequality in NumPy and many other Python projects.\n",
    "\n",
    "There is also a very interesting analysis by Reshama Shaikh on \"[Why Women Are Flourishing In R Community But Lagging In Python](https://reshamas.github.io/why-women-are-flourishing-in-r-community-but-lagging-in-python/)\" which contrasts the Python and R communities. I highly recommend reading Rashama's article, it has many good insights as to why R, in general, has succeeded in attracting a more diverse developer community.\n",
    "\n",
    "In my Computational Tools for Research in Biology course, I discuss [Git and Github](https://comptoolsres.github.io/TLCL_4.html) and the need for developer communities to be more inclusive of racial diversity, stop using offensive terms, and actively work to foster racial diversity. The same is true for gender diversity (Anthony's article also makes a great point about including non-binary people in assessment of diversity).\n",
    "\n",
    "While I am disappointed in the NumPy history and will encourage reforms, if we choose to stop using NumPy, we would not be able to use Python for a wide array (pun intended) of applications. So, we will use NumPy, but also commit to increasing diversity and acknowledge historical wrongs.\n",
    "\n",
    "# Introduction to NumPy\n",
    "\n",
    "This notebook is based on [chapter 2 of Jake VanderPlas' Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/02.00-introduction-to-numpy.html). [<img src=\"images/PDSH-cover-small.png\" alt=\"PDSH Cover Image\" style=\"width: 50px;float:right\"/>](https://jakevdp.github.io/PythonDataScienceHandbook/02.00-introduction-to-numpy.html)\n",
    "\n",
    "> This chapter, along with [chapter 3](https://jakevdp.github.io/PythonDataScienceHandbook/03.00-introduction-to-pandas.html), outlines techniques for effectively loading, storing, and manipulating in-memory data in Python. The topic is very broad: datasets can come from a wide range of sources and a wide range of formats, including be collections of documents, collections of images, collections of sound clips, collections of numerical measurements, or nearly anything else. **Despite this apparent heterogeneity, it will help us to think of all data fundamentally as arrays of numbers.**\n",
    "\n",
    "> For example, images–particularly **digital images**–can be thought of as simply two-dimensional arrays of numbers representing pixel brightness across the area. **Sound clips** can be thought of as one-dimensional arrays of intensity versus time. **Text** can be converted in various ways into numerical representations, perhaps binary digits representing the frequency of certain words or pairs of words. **No matter what the data are, the first step in making it analyzable will be to transform them into arrays of numbers.** (We will discuss some specific examples of this process later in [Feature Engineering](https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html))\n",
    "\n",
    "> For this reason, efficient storage and manipulation of numerical arrays is absolutely fundamental to the process of doing data science. We'll now take a look at the specialized tools that Python has for handling such numerical arrays: the NumPy package, and the Pandas package (discussed in Chapter 3).\n",
    "\n",
    "> This chapter will cover NumPy in detail. NumPy (short for **Numerical Python**) provides an efficient interface to store and operate on dense data buffers. In some ways, NumPy arrays are like Python's built-in list type, but **NumPy arrays provide much more efficient storage and data operations as the arrays grow larger in size.** NumPy arrays form the core of nearly the entire ecosystem of data science tools in Python, so time spent learning to use NumPy effectively will be valuable no matter what aspect of data science interests you.\n",
    "\n",
    "\n",
    "\n",
    "Now we can import and look at the version of NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> the __ methods of functions, like <code>__version__</code> are referred to as the \"double underscore\" or \"dunder\" methods and are generally not intended to be directly interacted with by users.\n",
    "</div>\n",
    "\n",
    "Remember the built in documentation with `<TAB>` and `?`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Data Types in Python\n",
    "\n",
    "As we've mentioned earlier, Python is **dynamically typed**. This flexibility can be handy, but, especially as sizes of datasets grow, if becomes a liability.\n",
    "\n",
    "As the PDSH chapter points out, languages such as C and Java are **statically typed**, meaning that the programmer has to declare, when the variable is created, the type of data that the variable will store.\n",
    "\n",
    "For example in C, you could make a loop like this:\n",
    "\n",
    "```C\n",
    "/* C code */\n",
    "int result = 0;\n",
    "for(int i=0; i<100; i++){\n",
    "    result += i;\n",
    "}\n",
    "```\n",
    "\n",
    "While in Python the same thing is done like this:\n",
    "\n",
    "```Python\n",
    "# Python code\n",
    "result = 0\n",
    "for i in range(100):\n",
    "    result += i\n",
    "```\n",
    "\n",
    "Notice that in C, the data types, all `int`s in the example, is explicitly declared, while in Python it is dynamically inferred.\n",
    "\n",
    "## A Python Integer is More Than Just and Integer\n",
    "\n",
    "We also briefly saw in the Intro to Jupyter session that a lot of the underlying code for Python is actually written in C (that is why the `??` function can't always display the code for a function). \n",
    "\n",
    "In the Intro to Python session, we learned about Object Oriented Programming, and that everything in Python is an object--even strings and integers.\n",
    "\n",
    "All of this leads to the reality that if we do something like `x=1000`, `x` is not just a \"raw\" integer--bits stored in memory. \n",
    "\n",
    "> It's actually a pointer to a compound C structure, which contains several values. Looking through the Python 3.4 source code, we find that the integer (long) type definition effectively looks like this (once the C macros are expanded):\n",
    "\n",
    "```C\n",
    "struct _longobject {\n",
    "    long ob_refcnt;\n",
    "    PyTypeObject *ob_type;\n",
    "    size_t ob_size;\n",
    "    long ob_digit[1];\n",
    "};\n",
    "```\n",
    "> A single integer in Python 3.4 actually contains four pieces:\n",
    "> * `ob_refcnt`, a reference count that helps Python silently handle memory allocation and deallocation\n",
    "> * `ob_type`, which encodes the type of the variable\n",
    "> * `ob_size`, which specifies the size of the following data members\n",
    "ob_digit, which contains the actual integer value that we expect the Python variable to represent.\n",
    "\n",
    "> This means that there is some overhead in storing an integer in Python as compared to an integer in a compiled language like C, as illustrated in the following figure:\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/cint_vs_pyint.png\" alt=\"Memory storage for C vs Python integers from Python Data Science Handbook\">\n",
    "  <figcaption>Memory storage for C vs Python integers, from Python Data Science Handbook</figcaption>\n",
    "</figure>\n",
    "\n",
    "> Here `PyObject_HEAD` is the part of the structure containing the reference count, type code, and other pieces mentioned before.\n",
    "\n",
    "> Notice the difference here: **a C integer is essentially a label for a position in memory whose bytes encode an integer value**. **A Python integer is a pointer to a position in memory containing all the Python object information**, including the bytes that contain the integer value. This extra information in the Python integer structure is what allows Python to be coded so freely and dynamically. All this additional information in Python types comes at a cost, however, which becomes especially apparent in structures that combine many of these objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Python List is More Than Just a List\n",
    "\n",
    "Practice creating some lists, and remember that a list can contain one or more data types--i.e. a Python list can have heterogenous data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some lists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As PDSH notes, this sets up the situation where each element of a list needs to store its own information about the element's data type:\n",
    "\n",
    "<figure>\n",
    "  <img src=\"images/PDSH_list.png\" alt=\"Python list image from Python Data Science Handbook\">\n",
    "  <figcaption>Memory storage for a Python list, from Python Data Science Handbook</figcaption>\n",
    "</figure>\n",
    "\n",
    "As you can imagine, this becomes exceedingly inefficient if, for example you have a list of 1,000,000 integers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Array from Python Lists\n",
    "\n",
    "PDSH mentions that there is a `array` module, but I rarely see anyone using it, so let's skip to the NumPy [`ndarry`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html)--a multi-dimensional array. NumPy not only procides the data structure, but also highly efficient operations on the data.\n",
    "\n",
    "First, we can create a NumPy array from a Python list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integer array from list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy arrays are constrained such that **all** elements need to be of the same type. \n",
    "\n",
    "If possible, NumPy will *upcast* items to create an array of matching type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array with a mix of integers and floats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about strings?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also specify the type if you want\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as the `ndarray` name implies, arrays can be multidimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Arrays from Scratch\n",
    "\n",
    "> Especially for larger arrays, it is more efficient to create arrays from scratch using routines built into NumPy. Here are several examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a length-10 integer array filled with zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3x5 floating-point array filled with ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array filled with a linear sequence\n",
    "# Starting at 0, ending at 20, stepping by 2\n",
    "# (this is similar to the built-in range() function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of five values evenly spaced between 0 and 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3x3 array of uniformly distributed random values between 0 and 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3x3 array of normally distributed random values\n",
    "# with mean 0 and standard deviation 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3x3 array of random integers in the interval [0, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3x3 identity matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an uninitialized array of three integers\n",
    "# The values will be whatever happens to already exist at that memory location\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Create the following types of NumPy arrays.\n",
    " * A 4X4 matrix with ones in every cell\n",
    " * A 6X6 matrix with ones on the diagonal from top left to bottom right.\n",
    " * A 3X3X3 matrix with normally distributed random numbers with mean of 5 and standard deciavion of 2\n",
    " * A vector with 1,000,000 evenly spaced numbers between five and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run the line below for a solution\n",
    "#%load snippets/NumPy_Ex_01.matrices.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Standard Data Types\n",
    "\n",
    "> NumPy arrays contain values of a single type, so it is important to have detailed knowledge of those types and their limitations. Because NumPy is built in C, the types will be familiar to users of C, Fortran, and other related languages.\n",
    "\n",
    "Notice that there are **a lot** of data types, and there are even more options if needed. This is one reason Python is handy. But again, that flexibility comes at a cost. \n",
    "\n",
    "Data type | Description\n",
    "----------|------------\n",
    "bool_ | Boolean (True or False) stored as a byte\n",
    "int_ | Default integer type (same as C long; normally either int64 or int32)\n",
    "intc | Identical to C int (normally int32 or int64)\n",
    "intp | Integer used for indexing (same as C ssize_t; normally either int32 or int64)\n",
    "int8 | Byte (-128 to 127)\n",
    "int16 | Integer (-32768 to 32767)\n",
    "int32 | Integer (-2147483648 to 2147483647)\n",
    "int64 | Integer (-9223372036854775808 to 9223372036854775807)\n",
    "uint8 | Unsigned integer (0 to 255)\n",
    "uint16 | Unsigned integer (0 to 65535)\n",
    "uint32 | Unsigned integer (0 to 4294967295)\n",
    "uint64 | Unsigned integer (0 to 18446744073709551615)\n",
    "float_ | Shorthand for float64.\n",
    "float16 | Half precision float: sign bit, 5 bits exponent, 10 bits mantissa\n",
    "float32 | Single precision float: sign bit, 8 bits exponent, 23 bits mantissa\n",
    "float64 | Double precision float: sign bit, 11 bits exponent, 52 bits mantissa\n",
    "complex_ | Shorthand for complex128.\n",
    "complex64 | Complex number, represented by two 32-bit floats\n",
    "complex128 | Complex number, represented by two 64-bit floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The bascis of NumPy Arrays\n",
    "\n",
    "> Data manipulation in Python is nearly synonymous with NumPy array manipulation: even newer tools like Pandas ([Chapter 3](https://jakevdp.github.io/PythonDataScienceHandbook/03.00-introduction-to-pandas.html)) are built around the NumPy array. Though note that as datasets grow, tools like Nvidia's [RAPIDS](https://rapids.ai/) framework are replacing Pandas by moving calculations onto the GPU, accelerating calculations. But, fear not, the Panadas and RAPIDS are largely compatible.\n",
    "\n",
    "\n",
    "### A note on random numbers\n",
    "\n",
    "PDSH moves on to creating three arrays to use for the following examples. Before we get there, let's take a look at the first thing that is done:\n",
    "\n",
    "`np.random.seed(0)`\n",
    "\n",
    "This sets the random number generator seed to 0. What does that mean?? Well, computers really can't make truely random numbers. What they use is a complex series of manipulations to generate numbers that apear random, sometimes called *pseudorandom*. If you start with the same number, the seed, the sequence of \"random\" numbers generated is **guaranteed** to be identical. This has good and bad properties. On the good side, we can set a seed and all have the same numbers, you can also use this for troubleshooting, etc. On the bad side, we are often lulled into a false sense of having simulated something repeatedly only to find that we failed to consider the biases that may be introduced by the random number generator--or worse, repeatedly simulating something using the same seed!\n",
    "\n",
    "Also, as a note, this guarantee only applies to identical code. PDSH used NumPy version 1.11.1, while we (using Python 3.8 full kerel on HiPerGator on 1/17/21) are using 1.13.1--while we will get consistent numbers from run to run and student to student, our numbers are different than in the text.\n",
    "\n",
    "### Create some arrays to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some sample arrays\n",
    "\n",
    "  # Set random number generator seed for reproducibility\n",
    "\n",
    "  # One-dimensional array\n",
    "  # Two-dimensional array\n",
    "  # Three-dimensional array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Array Attributes:\n",
    "\n",
    "> Each array has attributes `ndim` (the number of dimensions), `shape` (the size of each dimension), and `size` (the total size of the array):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the data type of the array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the itemsize and nbytes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array Indexing: Accessing Single Elements\n",
    "\n",
    "This is similar to using lists in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing from the end of the array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multi-dimensional arrays, items are accessed using a comma-separated list of indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array Slicing: Accessing Subarrays\n",
    "\n",
    "As with lists, NumPy array use slices.\n",
    "\n",
    "#### Slices for one-dimensional arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # first 5 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # elements from index 5 on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # middle sub-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # every other element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # every other element, starting at index 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A potentially confusing case is when the `step` value is negative. In this case, the defaults for `start` and `stop` are swapped. This becomes a convenient way to reverse an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # all elements, reversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # reversed every other from index 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slices for multi-dimensional subarrays\n",
    "\n",
    "> Multi-dimensional slices work in the same way, with multiple slices separated by commas. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # two rows, three columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # all rows, every other column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # first column of x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # first row of x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # equivalent to x2[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subarrays as no-copy views\n",
    "\n",
    "An important--and at times both usefull and confusing--thing to know about array slices is that thet return *views* rather than *copies* of the array data. Changing data in a subarray, changes the data in the originating array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a 2X2 subarray from this\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify element of x2_sub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This default behavior is actually quite useful: it means that when we work with large datasets, we can access and process pieces of these datasets without the need to copy the underlying data buffer.\n",
    "\n",
    "#### Creating copies of arrays\n",
    "\n",
    "If what you want is really a copy, you can use the `.copy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping Arrays\n",
    "\n",
    "Another common action is to reshape the dimensions of an array. The `.reshape()` method is the easiest way to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a row array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to column vector using newaxis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenation of arrays \n",
    "\n",
    "There are several functions to concatenate two arrays in NumPy: `np.concatenate`, `np.vstack`, and `np.hstack` are common methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating multiple arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating 2-dimensional arrays\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating along the second axis (zero-indexed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For mixed dimension arrays, vstack and hstack are more clear\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to be careful of dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split after 3rd and 5th elements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation on NumPy Arrays: Universal Functions\n",
    "\n",
    "> Up until now, we have been discussing some of the basic nuts and bolts of NumPy; in the next few sections, we will dive into the reasons that NumPy is so important in the Python data science world. Namely, it provides an easy and flexible interface to optimized computation with arrays of data.\n",
    "\n",
    "> Computation on NumPy arrays can be very fast, or it can be very slow. The key to making it fast is to use vectorized operations, generally implemented through NumPy's universal functions (ufuncs). This section motivates the need for NumPy's ufuncs, which can be used to make repeated calculations on array elements much more efficient. It then introduces many of the most common and useful arithmetic ufuncs available in the NumPy package.\n",
    "\n",
    "## The Slowness of Loops\n",
    "\n",
    "Both the dynamic typing and the interpreted nature of Python lead to slowness. PDSH talks about several options to circumvent some of this, and we will return to some throughout the semester.\n",
    "\n",
    "One thing to keep in ming though is that: \n",
    "\n",
    "> The relative sluggishness of Python generally manifests itself in situations where many small operations are being repeated – for instance looping over arrays to operate on each element. For example, imagine we have an array of values and we'd like to compute the reciprocal of each. A straightforward approach might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "def compute_reciprocals(values):\n",
    "    output = np.empty(len(values))\n",
    "    for i in range(len(values)):\n",
    "        output[i] = 1.0 / values[i]\n",
    "    return output\n",
    "        \n",
    "values = np.random.randint(1, 10, size=5)\n",
    "compute_reciprocals(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's time this on a big array:\n",
    "big_array = np.random.randint(1, 100, size=1000000)\n",
    "%timeit compute_reciprocals(big_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It turns out that the bottleneck here is not the operations themselves, but the type-checking and function dispatches that CPython must do at each cycle of the loop. Each time the reciprocal is computed, Python first examines the object's type and does a dynamic lookup of the correct function to use for that type. If we were working in compiled code instead, this type specification would be known before the code executes and the result could be computed much more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing UFuncs\n",
    "\n",
    "NumPy provides a convenient interface into a statically types, compiled routine in a **vectorized** operation.\n",
    "\n",
    "Let's compare the Python implementation to the UFunction that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think we can skip the rest of this section...there may be things we come back to, but I think this gets a bit into the weeds here.\n",
    "\n",
    "## Aggregations: Min, Max, and Everything In Between\n",
    "\n",
    "### Summing the Values in an Array\n",
    "\n",
    "Again the main take home here is that NumPy, both through its compiled code and its explicit typing, speeds up calculations. For example, suming a NumPy array of 1,000,000 random numbers can be done with both the built-in `sum()` function and the `np.sum()` function, which is much faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum and Maximum\n",
    "\n",
    "Similarly, the NumPy versions of these are faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For min, max, sum, and several other NumPy aggregates, a shorter syntax is to use methods of the array object itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Whenever possible, make sure that you are using the NumPy version of these aggregates when operating on NumPy arrays!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multidimensional aggregates\n",
    "\n",
    "For N-dimensional matrices, you can aggregate along different axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E.g. a two-dimensional matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, the aggregation is over the entire array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can specify the axis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The way the axis is specified here can be confusing to users coming from other languages. The `axis` keyword specifies the *dimension of the array that will be collapsed*, rather than the dimension that will be returned. So specifying `axis=0` means that the first axis will be collapsed: for two-dimensional arrays, this means that values within each column will be aggregated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UFRC Python-3.10",
   "language": "python",
   "name": "python3-3.10-ufrc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

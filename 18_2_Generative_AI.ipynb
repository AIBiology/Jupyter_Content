{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generative AI \n",
    "Generative AI refers to a subset of artificial intelligence models and algorithms designed to generate new data samples  that closely mirror the statistical properties of a target dataset. Unlike traditional classification and regression models, which predict labels or values based on input features, generative models aim to understand and replicate the complex distributions of input data, allowing them to produce entirely new data points that are similar or even indistinguishable to those in the training set.\n",
    "\n",
    "## Main Classes of Generative AI Models\n",
    "\n",
    "### 1. **Generative Adversarial Networks (GANs):**\n",
    "- **Overview:** Consist of two neural networks, the generator and the discriminator, that are trained simultaneously in an adversarial process.\n",
    "- **Applications:** Widely used for tasks like image generation, super-resolution, style transfer, and data augmentation.\n",
    "\n",
    "### 2. **Variational Autoencoders (VAEs):**\n",
    "- **Overview:** Based on Bayesian inference principles, VAEs encode input data into a distribution in latent space and decode from this space to reconstruct the input.\n",
    "- **Applications:** Employed in image generation, anomaly detection, and semi-supervised learning tasks.\n",
    "\n",
    "### 3. **Autoregressive Models:**\n",
    "- **Overview:** These models generate sequences by predicting each new piece of data conditioned on the previously generated pieces.\n",
    "- **Applications:** Utilized for text generation, image synthesis, and time series forecasting.\n",
    "\n",
    "### 4. **Diffusion Models:**\n",
    "- **Overview:** Generate data by gradually denoising a sample from a simple distribution over a series of steps, guided by a trained neural network.\n",
    "- **Applications:** Shown impressive results in high-fidelity image generation, audio synthesis, and molecular design.\n",
    "\n",
    "Each class has its strengths and is chosen based on specific task requirements, such as sample quality, training stability, and computational efficiency. The field continues to evolve rapidly, introducing new models and improving existing ones.\n",
    "\n",
    "## Differences from discriminative models\n",
    "In this course, at least so far, we have been focusing on class of models referred to as `discriminative models`. Deep learning classifiers and regressors are two examples of such models. Both classification and regression models focus on mapping input data to known outputs or labels, making them excellent for predictive tasks but not suited for generating new data.\n",
    "\n",
    "## Potential in Biology\n",
    "Generative models have significant potential in biology, offering innovative ways to tackle complex problems:\n",
    "\n",
    "* __Drug Discovery__: Generative models can propose novel molecular structures with desired properties, accelerating the identification of potential new drugs. See [examples](https://blogs.nvidia.com/blog/generative-ai-proteins-evozyne/)\n",
    "* __Synthetic Biology__: These models can design new genetic sequences or synthetic organisms with specific functions, supporting advances in bioengineering.\n",
    "* __Data Augmentation__: Generative AI can create additional training samples for rare conditions or species, enhancing the performance of classification models in biology.\n",
    "* __Understanding Complex Systems__: By generating data under different simulated conditions, generative models help in understanding complex biological systems and interactions.\n",
    "\n",
    "Today, we will concentrate in more mundane tasks, such as increasing the resolution of bioimages, or generating synthetic images. To do that, we will use a class of generative models called '_Generative Adversarial Networks_'.\n",
    "\n",
    "## Generative Adversarial Networks (GANs)\n",
    "GANs are a powerful class of generative models consisting of two neural networks, the generator and the discriminator, which are trained simultaneously in a competitive setting:\n",
    "\n",
    "* Generator: Learns to produce data that mimics the training dataset. In biology, this could involve generating synthetic genomic sequences or realistic cell images.\n",
    "* Discriminator: Learns to distinguish between real data from the training set and fake data produced by the generator. Its feedback helps improve the generator.\n",
    "The adversarial process leads to the generator creating highly realistic data, making GANs particularly effective for tasks requiring high-quality synthetic data generation. Please see the diagram below that was produced by [Google](https://developers.google.com/machine-learning/gan/gan_structure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GAN](images/gan_diagram.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating random digits\n",
    "\n",
    "Today, our first task will be to create a simple GAN implementation and train it to generate digit images that resemble those from the MNIST dataset, a collection of handwritten digits widely used in machine learning. The code we'll explore consists of two main components: the Generator, which will learn to produce images akin to MNIST digits, and the Discriminator, whose job is to distinguish between real MNIST images and the fakes produced by our Generator. As we dive into the code, keep in mind that the Generator and Discriminator are essentially in a continuous game of cat and mouse, improving through each iteration of our training loop. This dynamic interaction is what makes GANs both challenging and incredibly fascinating. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set keras backend to PyTorch\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'torch'\n",
    "\n",
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's load the MNIST dataset from torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "# Define the transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Download and load the MNIST dataset\n",
    "mnist_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Create the DataLoader\n",
    "train_loader = DataLoader(mnist_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define a simple generator model using Keras with PyTorch backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(7*7*128, use_bias=False, input_shape=(200,)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Reshape((7, 7, 128)),\n",
    "        # Upsample to 28x28x1\n",
    "        layers.Conv2DTranspose(1, (4, 4), strides=(4, 4), padding='same', use_bias=False, activation='tanh')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "generator = make_generator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the generator by defining a discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "discriminator = make_discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our losses (i.e., error metric). We will use binary cross entropy because the task at hand is quite simple. Either the model will consider a sample \"real\" or \"fake\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define loss functions\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    # Use BCEWithLogitsLoss which has better numerical stability\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    real_loss = bce(real_output, torch.ones_like(real_output))\n",
    "    fake_loss = bce(fake_output, torch.zeros_like(fake_output))\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    return bce(fake_output, torch.ones_like(fake_output))\n",
    "\n",
    "# Setup optimizers\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=5e-4)\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=5e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create some code to display images during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_and_display_images(model, test_input):\n",
    "    # Notice training=False so BatchNorm runs in inference mode\n",
    "    predictions = model(test_input, training=False)\n",
    "    predictions = predictions.detach().cpu().numpy()\n",
    "    \n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    \n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, we can define our approach for training the model and storing the losses/data as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Lists to store losses\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "\n",
    "def train(dataloader, epochs, seed, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Move seed to device\n",
    "    seed = seed.to(device)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(epochs), desc='Epochs'):\n",
    "        epoch_gen_loss = []\n",
    "        epoch_disc_loss = []\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            images, _ = batch\n",
    "            \n",
    "            # Ensure correct shape for MNIST images and move to device\n",
    "            images = images.reshape(-1, 28, 28, 1).to(device)\n",
    "            batch_size = images.shape[0]\n",
    "            \n",
    "            # Generate noise on device\n",
    "            noise = torch.randn(batch_size, 200, device=device)\n",
    "            \n",
    "            # ------- Combined forward pass (like TensorFlow version) -------\n",
    "            with autocast():\n",
    "                # Generate fake images once\n",
    "                fake_images = generator(noise, training=True)\n",
    "                \n",
    "                # Get discriminator outputs for both real and fake\n",
    "                real_output = discriminator(images, training=True)\n",
    "                fake_output = discriminator(fake_images.detach(), training=True)\n",
    "                \n",
    "                # Calculate discriminator loss\n",
    "                real_loss = nn.BCEWithLogitsLoss()(real_output, torch.ones_like(real_output))\n",
    "                fake_loss = nn.BCEWithLogitsLoss()(fake_output, torch.zeros_like(fake_output))\n",
    "                disc_loss = real_loss + fake_loss\n",
    "            \n",
    "            # ------- Update discriminator -------\n",
    "            discriminator_optimizer.zero_grad()\n",
    "            scaler.scale(disc_loss).backward()\n",
    "            scaler.step(discriminator_optimizer)\n",
    "            \n",
    "            # ------- Train generator with the same fake images -------\n",
    "            with autocast():\n",
    "                # Recompute discriminator output with the same fake images\n",
    "                # but allow gradients to flow to generator\n",
    "                fake_output = discriminator(fake_images, training=True)\n",
    "                \n",
    "                # Generator wants discriminator to think its images are real\n",
    "                gen_loss = nn.BCEWithLogitsLoss()(fake_output, torch.ones_like(fake_output))\n",
    "            \n",
    "            generator_optimizer.zero_grad()\n",
    "            scaler.scale(gen_loss).backward()\n",
    "            scaler.step(generator_optimizer)\n",
    "            \n",
    "            # Update scaler for next iteration\n",
    "            scaler.update()\n",
    "            \n",
    "            # Store losses\n",
    "            epoch_gen_loss.append(gen_loss.item())\n",
    "            epoch_disc_loss.append(disc_loss.item())\n",
    "        \n",
    "        # Average loss for the epoch\n",
    "        generator_losses.append(np.mean(epoch_gen_loss))\n",
    "        discriminator_losses.append(np.mean(epoch_disc_loss))\n",
    "        \n",
    "        # Display progress\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'\\nEpoch {epoch + 1} completed')\n",
    "            print(f'Generator loss: {generator_losses[-1]}, Discriminator loss: {discriminator_losses[-1]}')\n",
    "            \n",
    "            # Use autocast here too for consistency\n",
    "            with torch.no_grad(), autocast():\n",
    "                generate_and_display_images(generator, seed)\n",
    "    \n",
    "    # Final display\n",
    "    if epochs % 10 != 0:\n",
    "        with torch.no_grad(), autocast():\n",
    "            generate_and_display_images(generator, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create fixed noise for image generation\n",
    "seed = torch.randn(16, 200)  # 16 examples for a 4x4 grid\n",
    "EPOCHS = 100  # Set the number of epochs\n",
    "\n",
    "train(train_loader, EPOCHS, seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Applications: Super resolution!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the notebook demonstrates super-resolution using a GAN. We'll use a PyTorch implementation of ESRGAN (Enhanced Super-Resolution Generative Adversarial Networks) to recover high-resolution (HR) images from their low-resolution counterparts.\n",
    "\n",
    "The original model used in the TensorFlow version upsamples a 50x50 low resolution image to a 200x200 high resolution image (scale factor=4). For this PyTorch version, we'll implement a compatible interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining some convenience functions and importing the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualDenseBlock(nn.Module):\n",
    "    def __init__(self, nf=64, gc=32):\n",
    "        super(ResidualDenseBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(nf, gc, 3, 1, 1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(nf + gc, gc, 3, 1, 1, bias=True)\n",
    "        self.conv3 = nn.Conv2d(nf + 2 * gc, gc, 3, 1, 1, bias=True)\n",
    "        self.conv4 = nn.Conv2d(nf + 3 * gc, gc, 3, 1, 1, bias=True)\n",
    "        self.conv5 = nn.Conv2d(nf + 4 * gc, nf, 3, 1, 1, bias=True)\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.lrelu(self.conv1(x))\n",
    "        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))\n",
    "        x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))\n",
    "        x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))\n",
    "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
    "        return x5 * 0.2 + x\n",
    "\n",
    "class RRDB(nn.Module):\n",
    "    def __init__(self, nf=64):\n",
    "        super(RRDB, self).__init__()\n",
    "        self.rdb1 = ResidualDenseBlock(nf)\n",
    "        self.rdb2 = ResidualDenseBlock(nf)\n",
    "        self.rdb3 = ResidualDenseBlock(nf)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.rdb1(x)\n",
    "        out = self.rdb2(out)\n",
    "        out = self.rdb3(out)\n",
    "        return out * 0.2 + x\n",
    "\n",
    "class RRDBNet(nn.Module):\n",
    "    def __init__(self, in_nc=3, out_nc=3, nf=64, nb=23):\n",
    "        super(RRDBNet, self).__init__()\n",
    "        self.conv_first = nn.Conv2d(in_nc, nf, 3, 1, 1, bias=True)\n",
    "        \n",
    "        # RRDB blocks\n",
    "        self.body = nn.ModuleList()\n",
    "        for _ in range(nb):\n",
    "            self.body.append(RRDB(nf))\n",
    "            \n",
    "        self.conv_body = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        \n",
    "        # Upsampling\n",
    "        self.conv_up1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.conv_up2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.conv_hr = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.conv_last = nn.Conv2d(nf, out_nc, 3, 1, 1, bias=True)\n",
    "        \n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feat = self.conv_first(x)\n",
    "        body_feat = feat.clone()\n",
    "        \n",
    "        for block in self.body:\n",
    "            body_feat = block(body_feat)\n",
    "            \n",
    "        body_feat = self.conv_body(body_feat)\n",
    "        feat = feat + body_feat\n",
    "        \n",
    "        # Upsampling\n",
    "        feat = self.lrelu(F.interpolate(self.conv_up1(feat), scale_factor=2, mode='nearest'))\n",
    "        feat = self.lrelu(F.interpolate(self.conv_up2(feat), scale_factor=2, mode='nearest'))\n",
    "        \n",
    "        feat = self.lrelu(self.conv_hr(feat))\n",
    "        out = self.conv_last(feat)\n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our super-resolution wrapper class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESRGANUpscaler:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize the model with correct architecture\n",
    "        self.model = RRDBNet(in_nc=3, out_nc=3, nf=64, nb=23).to(self.device)\n",
    "        \n",
    "        # Download weights from GitHub if not present\n",
    "        model_path = 'RealESRGAN_x4plus.pth'\n",
    "        if not os.path.exists(model_path):\n",
    "            print(\"Downloading pre-trained ESRGAN model...\")\n",
    "            url = 'https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth'\n",
    "            r = requests.get(url)\n",
    "            with open(model_path, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "            print(f\"Downloaded model to {model_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Load pre-trained weights\n",
    "            weights = torch.load(model_path, map_location=self.device)\n",
    "            \n",
    "            # Extract the appropriate part of the state dict\n",
    "            if 'params_ema' in weights:\n",
    "                state_dict = weights['params_ema']\n",
    "            elif 'params' in weights:\n",
    "                state_dict = weights['params']\n",
    "            else:\n",
    "                state_dict = weights\n",
    "                \n",
    "            self.model.load_state_dict(state_dict, strict=True)\n",
    "            self.model.eval()\n",
    "            print(\"Successfully loaded pre-trained ESRGAN model!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            print(\"Using model without pre-trained weights (results will be poor)\")\n",
    "        \n",
    "    def upscale(self, img):\n",
    "        \"\"\"\n",
    "        Upscale an image using the ESRGAN model\n",
    "        Args:\n",
    "            img: PIL Image or numpy array\n",
    "        Returns:\n",
    "            super-resolution image as numpy array\n",
    "        \"\"\"\n",
    "        # Convert input to proper format\n",
    "        if isinstance(img, np.ndarray):\n",
    "            img = Image.fromarray(img.astype(np.uint8))\n",
    "        \n",
    "        # Ensure it's RGB\n",
    "        img = img.convert('RGB')\n",
    "        \n",
    "        # Convert to tensor and normalize to [0, 1]\n",
    "        img_tensor = torch.from_numpy(np.array(img)).float().div(255.0)\n",
    "        # Change from HWC to CHW format\n",
    "        img_tensor = img_tensor.permute(2, 0, 1).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # Process with the model\n",
    "        with torch.no_grad():\n",
    "            if self.device.type == 'cuda':\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    output = self.model(img_tensor)\n",
    "            else:\n",
    "                output = self.model(img_tensor)\n",
    "        \n",
    "        # Convert output tensor to numpy array\n",
    "        output = output.squeeze().float().cpu().clamp_(0, 1).permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Scale to [0, 255] and convert to uint8\n",
    "        output = (output * 255.0).round().astype(np.uint8)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's download and display a test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "url = 'https://raw.githubusercontent.com/tensorflow/examples/master/lite/examples/super_resolution/android/app/src/main/assets/lr-1.jpg'\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Hide axes ticks\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try to upsample this image using interpolation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w, h = img.size\n",
    "bicubic = np.array(img.resize((w*4, h*4), Image.BICUBIC))\n",
    "\n",
    "# Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Bicubic')\n",
    "plt.imshow(bicubic)  # Convert CHW -> HWC for matplotlib\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't look that great. Finally, we can run the model and see if we obtain a better image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ESRGAN upscaler\n",
    "print(\"Initializing ESRGAN model...\")\n",
    "esrgan_model = ESRGANUpscaler()\n",
    "\n",
    "# Upscale with ESRGAN\n",
    "print(\"Upscaling with ESRGAN...\")\n",
    "esrgan_result = esrgan_model.upscale(img)\n",
    "\n",
    "# Create a bicubic upscaled version for comparison\n",
    "w, h = img.size\n",
    "bicubic = np.array(img.resize((w*4, h*4), Image.BICUBIC))\n",
    "\n",
    "# Display results\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Original')\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Bicubic (4x)')\n",
    "plt.imshow(bicubic)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('ESRGAN (4x)')\n",
    "plt.imshow(esrgan_result)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications: Image Synthesis\n",
    "Generative Adversarial Networks (GANs) are powerful models for synthesizing new, realistic images from random noise inputs. In this section, we demonstrate how an old-school (2015) pretrained GAN generator can be used to produce synthetic images, illustrating the generative capability central to GAN-based synthesis tasks.\n",
    "\n",
    "Introduced by Radford et al. in 2015, DCGAN demonstrated that GANs could generate high-quality, coherent images by leveraging convolutional neural networks (CNNs) in both the generator and discriminator components. This marked a significant advancement over earlier fully connected GAN architectures, which struggled to capture spatial hierarchies in image data.\n",
    "\n",
    "### Why DCGAN?\n",
    "\n",
    "DCGAN introduced key architectural innovations that made GANs more stable and efficient to train:\n",
    "\n",
    "* Convolutional layers without pooling: Both the generator and discriminator use strided convolutions, removing the need for pooling layers and allowing the network to learn its own spatial downsampling or upsampling.\n",
    "\n",
    "* Batch normalization: Applied in both networks (except in the output layers), it stabilizes training by normalizing feature distributions.\n",
    "\n",
    "* ReLU and LeakyReLU activations: The generator uses ReLU activations, encouraging diverse outputs, while the discriminator uses LeakyReLU to avoid dead neurons.\n",
    "\n",
    "* Tanh output: The generator outputs images in a normalized range (-1, 1), improving convergence.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "\n",
    "# Define your Generator \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz=100, ngf=64, nc=3):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),   # main.0\n",
    "            nn.BatchNorm2d(ngf * 8),                               # main.1\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False), # main.3\n",
    "            nn.BatchNorm2d(ngf * 4),                               # main.4\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False), # main.6\n",
    "            nn.BatchNorm2d(ngf * 2),                               # main.7\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),     # main.9\n",
    "            nn.BatchNorm2d(ngf),                                   # main.10\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),          # main.12\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# Download the model\n",
    "model_url = 'https://github.com/Natsu6767/DCGAN-PyTorch/raw/master/model/model_final.pth'\n",
    "model_filename = 'model_final.pth'\n",
    "urllib.request.urlretrieve(model_url, model_filename)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(model_filename, map_location='cpu')\n",
    "gen_weights = checkpoint['generator']\n",
    "\n",
    "# Remap keys\n",
    "new_state_dict = {}\n",
    "key_map = {\n",
    "    'tconv1.weight': 'main.0.weight',\n",
    "    'bn1.weight': 'main.1.weight',\n",
    "    'bn1.bias': 'main.1.bias',\n",
    "    'bn1.running_mean': 'main.1.running_mean',\n",
    "    'bn1.running_var': 'main.1.running_var',\n",
    "    'tconv2.weight': 'main.3.weight',\n",
    "    'bn2.weight': 'main.4.weight',\n",
    "    'bn2.bias': 'main.4.bias',\n",
    "    'bn2.running_mean': 'main.4.running_mean',\n",
    "    'bn2.running_var': 'main.4.running_var',\n",
    "    'tconv3.weight': 'main.6.weight',\n",
    "    'bn3.weight': 'main.7.weight',\n",
    "    'bn3.bias': 'main.7.bias',\n",
    "    'bn3.running_mean': 'main.7.running_mean',\n",
    "    'bn3.running_var': 'main.7.running_var',\n",
    "    'tconv4.weight': 'main.9.weight',\n",
    "    'bn4.weight': 'main.10.weight',\n",
    "    'bn4.bias': 'main.10.bias',\n",
    "    'bn4.running_mean': 'main.10.running_mean',\n",
    "    'bn4.running_var': 'main.10.running_var',\n",
    "    'tconv5.weight': 'main.12.weight'\n",
    "}\n",
    "\n",
    "for k, v in gen_weights.items():\n",
    "    if k in key_map:\n",
    "        new_state_dict[key_map[k]] = v\n",
    "    else:\n",
    "        print(f\"Skipping key: {k}\")\n",
    "\n",
    "# Initialize generator\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "netG = Generator().to(device)\n",
    "netG.load_state_dict(new_state_dict)\n",
    "netG.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images\n",
    "nz = 100\n",
    "num_images = 4\n",
    "noise = torch.randn(num_images, nz, 1, 1, device=device)\n",
    "with torch.no_grad():\n",
    "    fake_images = netG(noise).cpu()\n",
    "\n",
    "# Plot\n",
    "grid = vutils.make_grid(fake_images, nrow=4, normalize=True)\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.imshow(grid.permute(1, 2, 0).numpy())\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Biology",
   "language": "python",
   "name": "aibio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
